{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delayed tasks analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert notebook to python\n",
    "```\n",
    "bash\n",
    "jupyter nbconvert \"D:\\OneDrive - Nexus365\\Private_Dropbox\\Projects\\trialexp\\notebooks\\noncanonical\\nb20230202_145400_delayed_tasks_and_316_photometry.ipynb\" --to=\"python\" --output-dir=\"D:\\OneDrive - Nexus365\\Private_Dropbox\\Projects\\trialexp\\notebooks\\noncanonical\" --output=\"nb20230202_145400_delayed_tasks_and_316_photometry\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick analysis of instrumental reaching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : sktime package not installed, this is `only an issue\n",
      "        if you want to export datasets to perform Maching Learning tasks.\n",
      "        To solve, type pip install sktime in your environment\n"
     ]
    }
   ],
   "source": [
    "# allow for automatic reloading of classes and function when updating the code\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "# Import Session and Experiment class with helper functions\n",
    "from trialexp.process.data_import import *\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "trial_window = [-2000, 6000] # in ms\n",
    "\n",
    "# time limit around trigger to perform an event\n",
    "# determine successful trials\n",
    "# timelim = [1000, 4000] # in ms\n",
    "\n",
    "# Digital channel nb of the pyphotometry device\n",
    "# on which rsync signal is sent (from pycontrol device)\n",
    "rsync_chan = 2\n",
    "\n",
    "basefolder = Path(os.getcwd()).parent.parent\n",
    "\n",
    "# These must be absolute paths\n",
    "# use this to use within package tasks files (in params)\n",
    "tasksfile = Path(basefolder,'params','tasks_params.csv')\n",
    "# use this to put a local full path\n",
    "#tasksfile = -r'C:/.../tasks_params.csv' \n",
    "\n",
    "# from sample_data\n",
    "\n",
    "# # From jade\n",
    "# photometry_dir = Path('/home/MRC.OX.AC.UK/phar0732/ettin/Data/head-fixed/photometry')\n",
    "# pycontrol_dir = Path('/home/MRC.OX.AC.UK/phar0732/ettin/Data/head-fixed/pycontrol')\n",
    "\n",
    "# From julien-pc\n",
    "photometry_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\pyphotometry\\data\\reaching_go_spout_bar_nov22'\n",
    "pycontrol_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\pycontrol\\reaching_go_spout_bar_nov22'\n",
    "\n",
    "#From laptop\n",
    "# photometry_dir = r'C:\\Users\\phar0732\\Documents\\GitHub\\trialexp\\sample_data\\pyphotometry\\reaching_go_spout_incr_break2_nov22'\n",
    "# pycontrol_dir = r'C:\\Users\\phar0732\\Documents\\GitHub\\trialexp\\sample_data\\pycontrol\\reaching_go_spout_incr_break2_nov22'\n",
    "\n",
    "video_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\videos'\n",
    "tasks = pd.read_csv(tasksfile, usecols=[1, 2, 3, 4], index_col=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sessions loaded from: sessions.pkl\n",
      "Unable to import file: v-2022-11-26-150859.txt\n",
      "invalid literal for int() with base 10: ''\n",
      "session nb:  1 0 2022-11-19 17:58:51 reaching_go_spout_bar_nov22\n",
      "session nb:  2 0 2022-11-19 17:59:45 reaching_go_spout_bar_nov22\n",
      "session nb:  3 0 2022-11-19 18:10:19 reaching_go_spout_bar_nov22\n",
      "session nb:  4 0 2022-11-19 18:17:41 reaching_go_spout_bar_nov22\n",
      "session nb:  5 0 2022-11-19 18:19:36 reaching_go_spout_bar_nov22\n",
      "session nb:  6 0 2022-11-19 18:25:34 reaching_go_spout_bar_nov22\n",
      "session nb:  7 0 2022-11-19 18:26:09 reaching_go_spout_bar_nov22\n",
      "session nb:  8 0 2022-11-19 18:26:36 reaching_go_spout_bar_nov22\n",
      "session nb:  9 0 2022-11-19 18:35:34 reaching_go_spout_bar_nov22\n",
      "session nb:  10 0 2022-11-19 18:38:56 reaching_go_spout_bar_nov22\n",
      "session nb:  11 0 2022-11-19 18:40:03 reaching_go_spout_bar_nov22\n",
      "session nb:  12 0 2022-11-19 18:41:07 reaching_go_spout_bar_nov22\n",
      "session nb:  13 0 2022-11-21 14:00:02 reaching_go_spout_nov22\n",
      "session nb:  14 0 2022-11-21 15:11:28 reaching_go_spout_nov22\n",
      "session nb:  15 0 2022-11-21 15:12:53 reaching_go_spout_nov22\n",
      "session nb:  16 0 2022-11-21 16:52:57 reaching_go_spout_nov22\n",
      "session nb:  17 0 2022-11-21 16:59:44 reaching_go_spout_nov22\n",
      "session nb:  18 0 2022-11-21 17:02:14 reaching_go_spout_nov22\n",
      "session nb:  19 0 2022-11-21 17:05:55 reaching_go_spout_nov22\n",
      "session nb:  20 0 2022-11-21 17:06:38 reaching_go_spout_nov22\n",
      "session nb:  21 0 2022-11-21 17:06:58 reaching_go_spout_nov22\n",
      "session nb:  22 0 2022-11-23 14:34:57 reaching_go_spout_nov22\n",
      "session nb:  23 0 2022-11-23 14:35:56 reaching_go_spout_bar_nov22\n",
      "session nb:  24 0 2022-11-23 14:38:18 reaching_go_spout_bar_nov22\n",
      "session nb:  25 0 2022-11-23 14:44:03 reaching_go_spout_bar_nov22\n",
      "session nb:  26 0 2022-11-23 17:37:48 reaching_go_spout_bar_nov22\n",
      "session nb:  27 0 2022-11-24 18:28:31 reaching_go_spout_bar_nov22\n",
      "session nb:  28 0 2022-11-24 18:29:50 reaching_go_spout_bar_nov22\n",
      "session nb:  29 0 2022-11-24 18:53:46 reaching_go_spout_bar_nov22\n",
      "session nb:  30 0 2022-11-25 11:18:19 reaching_go_spout_bar_nov22\n",
      "session nb:  31 0 2022-11-25 11:20:24 reaching_go_spout_bar_nov22\n",
      "session nb:  32 0 2022-11-25 11:22:37 reaching_go_spout_bar_nov22\n",
      "session nb:  33 0 2023-02-16 09:56:48 reaching_go_spout_bar_nov22\n",
      "session nb:  34 0 2023-03-09 10:45:14 reaching_go_spout_bar_nov22\n",
      "session nb:  1 1 2022-11-24 18:28:31 reaching_go_spout_bar_nov22\n",
      "session nb:  2 1 2022-11-24 18:29:50 reaching_go_spout_bar_nov22\n",
      "session nb:  3 1 2022-11-24 18:53:47 reaching_go_spout_bar_nov22\n",
      "session nb:  4 1 2022-11-25 11:20:24 reaching_go_spout_bar_nov22\n",
      "session nb:  5 1 2022-11-25 11:22:37 reaching_go_spout_bar_nov22\n",
      "session nb:  1 64 2023-02-02 11:18:05 reaching_go_spout_bar_nov22\n",
      "session nb:  2 64 2023-02-03 15:53:28 reaching_go_spout_bar_nov22\n",
      "session nb:  3 64 2023-02-03 15:57:49 reaching_go_spout_bar_nov22\n",
      "session nb:  4 64 2023-02-06 09:44:31 reaching_go_spout_bar_nov22\n",
      "session nb:  5 64 2023-02-07 10:20:29 reaching_go_spout_bar_nov22\n",
      "session nb:  6 64 2023-02-08 10:04:49 reaching_go_spout_bar_nov22\n",
      "session nb:  7 64 2023-02-09 09:53:17 reaching_go_spout_bar_nov22\n",
      "session nb:  8 64 2023-02-10 11:55:09 reaching_go_spout_bar_nov22\n",
      "session nb:  9 64 2023-02-10 12:36:20 reaching_go_spout_bar_nov22\n",
      "session nb:  10 64 2023-02-13 10:49:49 reaching_go_spout_bar_nov22\n",
      "session nb:  11 64 2023-02-14 11:50:18 reaching_go_spout_bar_nov22\n",
      "session nb:  12 64 2023-02-15 10:44:38 reaching_go_spout_bar_nov22\n",
      "session nb:  13 64 2023-02-16 10:34:24 reaching_go_spout_bar_nov22\n",
      "session nb:  14 64 2023-02-17 09:49:40 reaching_go_spout_bar_nov22\n",
      "session nb:  15 64 2023-02-17 09:49:40 reaching_go_spout_bar_nov22\n",
      "session nb:  16 64 2023-02-20 10:13:08 reaching_go_spout_bar_nov22\n",
      "session nb:  17 64 2023-02-21 10:29:55 reaching_go_spout_bar_nov22\n",
      "session nb:  18 64 2023-02-22 15:09:06 reaching_go_spout_bar_nov22\n",
      "session nb:  19 64 2023-02-23 14:38:46 reaching_go_spout_bar_nov22\n",
      "session nb:  20 64 2023-02-23 15:18:59 reaching_go_spout_bar_nov22\n",
      "session nb:  21 64 2023-02-24 09:56:57 reaching_go_spout_bar_nov22\n",
      "session nb:  22 64 2023-02-24 10:38:31 reaching_go_spout_bar_nov22\n",
      "session nb:  23 64 2023-02-27 16:19:56 reaching_go_spout_bar_nov22\n",
      "session nb:  24 64 2023-02-27 16:37:09 reaching_go_spout_bar_nov22\n",
      "session nb:  25 64 2023-02-28 09:47:11 reaching_go_spout_bar_nov22\n",
      "session nb:  26 64 2023-03-01 10:25:03 reaching_go_spout_bar_nov22\n",
      "session nb:  27 64 2023-03-02 10:23:11 reaching_go_spout_bar_nov22\n",
      "session nb:  28 64 2023-03-02 10:35:47 reaching_go_spout_nov22\n",
      "session nb:  29 64 2023-03-02 10:39:14 reaching_go_spout_incr_break2_nov22\n",
      "session nb:  30 64 2023-03-03 10:29:30 reaching_go_spout_incr_break2_nov22\n",
      "session nb:  31 64 2023-03-06 10:45:12 reaching_go_spout_bar_nov22\n",
      "session nb:  32 64 2023-03-06 10:54:05 reaching_go_spout_incr_break2_nov22\n",
      "session nb:  33 64 2023-03-06 11:01:46 reaching_go_spout_incr_break2_nov22\n",
      "session nb:  34 64 2023-03-06 11:36:52 reaching_go_spout_bar_nov22\n",
      "session nb:  1 603 2023-03-21 15:29:00 reaching_go_spout_bar_mar23\n",
      "session nb:  2 603 2023-03-22 13:01:21 reaching_go_spout_bar_mar23\n",
      "session nb:  3 603 2023-03-23 11:23:24 reaching_go_spout_bar_mar23\n",
      "session nb:  4 603 2023-03-24 08:39:53 reaching_go_spout_bar_mar23\n",
      "session nb:  5 603 2023-03-27 15:00:06 reaching_go_spout_bar_mar23\n",
      "session nb:  6 603 2023-03-29 08:38:20 reaching_go_spout_bar_mar23\n",
      "session nb:  7 603 2023-03-29 19:49:34 reaching_go_spout_bar_mar23\n",
      "session nb:  1 58 2023-02-02 11:28:09 reaching_go_spout_bar_nov22\n",
      "session nb:  2 58 2023-02-03 15:58:23 reaching_go_spout_bar_nov22\n",
      "session nb:  3 58 2023-02-20 12:04:53 reaching_go_spout_bar_nov22\n",
      "session nb:  4 58 2023-02-21 11:52:13 reaching_go_spout_bar_nov22\n",
      "session nb:  5 58 2023-02-22 16:21:47 reaching_go_spout_bar_nov22\n",
      "session nb:  6 58 2023-02-23 16:32:36 reaching_go_spout_bar_nov22\n",
      "session nb:  7 58 2023-02-23 17:13:13 reaching_go_spout_bar_nov22\n",
      "session nb:  8 58 2023-02-24 11:15:21 reaching_go_spout_bar_nov22\n",
      "session nb:  9 58 2023-02-24 11:57:00 reaching_go_spout_bar_nov22\n",
      "session nb:  10 58 2023-02-28 14:41:01 reaching_go_spout_bar_nov22\n",
      "session nb:  11 58 2023-03-01 11:14:46 reaching_go_spout_bar_nov22\n",
      "session nb:  12 58 2023-03-02 10:21:29 reaching_go_spout_bar_nov22\n",
      "session nb:  13 58 2023-03-03 11:01:47 reaching_go_spout_bar_nov22\n",
      "session nb:  14 58 2023-03-03 11:13:20 reaching_go_spout_bar_nov22\n",
      "session nb:  15 58 2023-03-06 11:37:03 reaching_go_spout_bar_nov22\n",
      "session nb:  16 58 2023-03-06 11:54:58 reaching_go_spout_bar_nov22\n",
      "session nb:  17 58 2023-03-07 10:43:32 reaching_go_spout_bar_nov22\n",
      "session nb:  18 58 2023-03-09 10:47:59 reaching_go_spout_bar_nov22\n",
      "session nb:  19 58 2023-03-10 10:17:10 reaching_go_spout_bar_nov22\n",
      "session nb:  20 58 2023-03-13 10:19:15 reaching_go_spout_bar_nov22\n",
      "session nb:  21 58 2023-03-14 10:11:59 reaching_go_spout_bar_nov22\n",
      "session nb:  22 58 2023-03-14 16:51:10 reaching_go_spout_bar_mar23\n",
      "session nb:  23 58 2023-03-15 11:43:06 reaching_go_spout_bar_mar23\n",
      "session nb:  24 58 2023-03-16 11:01:34 reaching_go_spout_bar_mar23\n",
      "session nb:  25 58 2023-03-17 08:43:07 reaching_go_spout_bar_mar23\n",
      "session nb:  26 58 2023-03-18 17:10:32 reaching_go_spout_bar_mar23\n",
      "session nb:  27 58 2023-03-19 15:13:57 reaching_go_spout_bar_mar23\n",
      "session nb:  28 58 2023-03-20 13:26:58 reaching_go_spout_bar_mar23\n",
      "session nb:  29 58 2023-03-22 18:26:17 reaching_go_spout_bar_mar23\n",
      "session nb:  30 58 2023-03-23 19:17:40 reaching_go_spout_bar_mar23\n",
      "session nb:  31 58 2023-03-24 15:12:54 reaching_go_spout_bar_mar23\n",
      "session nb:  32 58 2023-03-25 18:40:34 reaching_go_spout_bar_mar23\n",
      "session nb:  1 604 2023-03-14 09:25:22 reaching_go_spout_bar_mar23\n",
      "session nb:  2 604 2023-03-14 09:30:01 reaching_go_spout_bar_mar23\n",
      "session nb:  3 604 2023-03-15 11:48:19 reaching_go_spout_bar_mar23\n",
      "session nb:  4 604 2023-03-16 10:24:19 reaching_go_spout_bar_mar23\n",
      "session nb:  5 604 2023-03-17 09:01:13 reaching_go_spout_bar_mar23\n",
      "session nb:  6 604 2023-03-20 09:48:35 reaching_go_spout_bar_mar23\n",
      "session nb:  7 604 2023-03-21 15:45:47 reaching_go_spout_bar_mar23\n",
      "session nb:  8 604 2023-03-22 12:03:36 reaching_go_spout_bar_mar23\n",
      "session nb:  9 604 2023-03-23 09:03:42 reaching_go_spout_bar_mar23\n",
      "session nb:  10 604 2023-03-24 09:11:34 reaching_go_spout_bar_mar23\n",
      "session nb:  11 604 2023-03-27 09:12:07 reaching_go_spout_bar_mar23\n",
      "session nb:  12 604 2023-03-29 09:11:33 reaching_go_spout_bar_mar23\n",
      "session nb:  13 604 2023-03-29 16:14:31 reaching_go_spout_bar_mar23\n",
      "session nb:  14 604 2023-03-29 16:23:24 reaching_go_spout_bar_mar23\n",
      "session nb:  1 61 2023-02-17 10:57:22 reaching_go_spout_bar_nov22\n",
      "session nb:  2 61 2023-02-17 11:14:23 reaching_go_spout_bar_nov22\n",
      "session nb:  3 61 2023-02-21 12:11:10 reaching_go_spout_bar_nov22\n",
      "session nb:  4 61 2023-03-03 10:58:48 reaching_go_spout_incr_break2_nov22\n",
      "session nb:  5 61 2023-03-06 11:43:53 reaching_go_spout_bar_nov22\n",
      "session nb:  6 61 2023-03-06 12:12:46 reaching_go_spout_bar_nov22\n",
      "session nb:  7 61 2023-03-07 11:50:26 reaching_go_spout_bar_nov22\n",
      "session nb:  8 61 2023-03-09 11:52:10 reaching_go_spout_bar_nov22\n",
      "session nb:  9 61 2023-03-10 11:39:26 reaching_go_spout_bar_nov22\n",
      "session nb:  10 61 2023-03-13 16:54:20 reaching_go_spout_bar_mar23\n",
      "session nb:  11 61 2023-03-14 11:03:25 reaching_go_spout_bar_mar23\n",
      "session nb:  12 61 2023-03-15 10:55:38 reaching_go_spout_bar_mar23\n",
      "session nb:  1 602 2023-03-16 09:19:35 reaching_go_spout_bar_mar23\n",
      "session nb:  2 602 2023-03-16 09:39:26 reaching_go_spout_bar_mar23\n",
      "session nb:  3 602 2023-03-17 09:23:07 reaching_go_spout_bar_nov22\n",
      "session nb:  4 602 2023-03-17 15:07:53 reaching_go_spout_bar_mar23\n",
      "session nb:  5 602 2023-03-20 14:56:14 reaching_go_spout_bar_mar23\n",
      "session nb:  6 602 2023-03-21 14:36:10 reaching_go_spout_bar_mar23\n",
      "session nb:  7 602 2023-03-22 12:14:14 reaching_go_spout_bar_mar23\n",
      "session nb:  8 602 2023-03-22 12:40:29 reaching_go_spout_bar_mar23\n",
      "session nb:  9 602 2023-03-23 08:51:41 reaching_go_spout_bar_mar23\n",
      "session nb:  1 62 2023-02-08 15:22:34 reaching_go_spout_bar_nov22\n",
      "session nb:  2 62 2023-02-09 10:02:52 reaching_go_spout_bar_nov22\n",
      "session nb:  3 62 2023-02-10 12:22:10 reaching_go_spout_bar_nov22\n",
      "session nb:  4 62 2023-02-13 11:02:11 reaching_go_spout_bar_nov22\n",
      "session nb:  5 62 2023-02-14 12:20:51 reaching_go_spout_bar_nov22\n",
      "session nb:  6 62 2023-02-15 10:28:05 reaching_go_spout_bar_nov22\n",
      "session nb:  7 62 2023-02-16 11:09:11 reaching_go_spout_bar_nov22\n",
      "session nb:  8 62 2023-02-17 09:59:47 reaching_go_spout_bar_nov22\n",
      "session nb:  9 62 2023-02-20 10:36:33 reaching_go_spout_bar_nov22\n",
      "session nb:  10 62 2023-02-21 10:34:00 reaching_go_spout_bar_nov22\n",
      "session nb:  11 62 2023-02-22 15:08:28 reaching_go_spout_bar_nov22\n",
      "session nb:  12 62 2023-02-23 14:33:55 reaching_go_spout_bar_nov22\n",
      "session nb:  13 62 2023-02-24 09:58:22 reaching_go_spout_bar_nov22\n",
      "session nb:  14 62 2023-02-28 09:31:37 reaching_go_spout_bar_nov22\n",
      "session nb:  15 62 2023-03-01 09:32:23 reaching_go_spout_bar_nov22\n",
      "session nb:  16 62 2023-03-02 09:26:42 reaching_go_spout_bar_nov22\n",
      "session nb:  17 62 2023-03-03 09:12:32 reaching_go_spout_bar_nov22\n",
      "session nb:  18 62 2023-03-04 17:22:58 reaching_go_spout_bar_nov22\n",
      "session nb:  19 62 2023-03-05 18:34:08 reaching_go_spout_bar_nov22\n",
      "session nb:  20 62 2023-03-06 18:23:44 reaching_go_spout_bar_nov22\n",
      "session nb:  21 62 2023-03-07 18:34:22 reaching_go_spout_bar_nov22\n",
      "session nb:  1 313 2022-11-24 11:32:33 reaching_go_spout_bar_nov22\n",
      "session nb:  2 313 2022-11-25 11:33:18 reaching_go_spout_bar_nov22\n",
      "session nb:  3 313 2022-11-25 14:37:07 reaching_go_spout_bar_nov22\n",
      "session nb:  4 313 2022-11-28 15:19:34 reaching_go_spout_bar_nov22\n",
      "session nb:  5 313 2022-11-29 11:31:47 reaching_go_spout_bar_nov22\n",
      "session nb:  6 313 2022-11-30 15:28:46 reaching_go_spout_bar_nov22\n",
      "session nb:  7 313 2022-12-01 11:17:17 reaching_go_spout_bar_nov22\n",
      "session nb:  8 313 2022-12-01 11:58:04 reaching_go_spout_bar_nov22\n",
      "session nb:  9 313 2022-12-02 13:45:33 reaching_go_spout_bar_nov22\n",
      "session nb:  10 313 2022-12-07 13:33:28 reaching_go_spout_bar_nov22\n",
      "session nb:  11 313 2022-12-07 13:42:49 reaching_go_spout_bar_nov22\n",
      "session nb:  12 313 2022-12-07 14:06:29 reaching_go_spout_bar_nov22\n",
      "session nb:  13 313 2022-12-08 19:29:06 reaching_go_spout_bar_nov22\n",
      "session nb:  14 313 2022-12-08 19:45:30 reaching_go_spout_bar_nov22\n",
      "session nb:  15 313 2022-12-09 16:02:01 reaching_go_spout_bar_nov22\n",
      "session nb:  16 313 2022-12-10 18:26:19 reaching_go_spout_bar_nov22\n",
      "session nb:  17 313 2022-12-10 18:45:12 reaching_go_spout_bar_nov22\n",
      "session nb:  18 313 2022-12-12 14:31:40 reaching_go_spout_bar_nov22\n",
      "session nb:  19 313 2022-12-12 14:47:42 reaching_go_spout_bar_nov22\n",
      "session nb:  20 313 2022-12-13 11:29:05 reaching_go_spout_bar_nov22\n",
      "session nb:  21 313 2022-12-14 14:35:25 reaching_go_spout_bar_nov22\n",
      "session nb:  1 314 2022-11-24 11:14:52 reaching_go_spout_bar_nov22\n",
      "session nb:  2 314 2022-11-24 11:32:34 reaching_go_spout_bar_nov22\n",
      "session nb:  3 314 2022-11-25 11:33:18 reaching_go_spout_bar_nov22\n",
      "session nb:  4 314 2022-11-25 14:37:07 reaching_go_spout_bar_nov22\n",
      "session nb:  5 314 2022-11-28 15:19:34 reaching_go_spout_bar_nov22\n",
      "session nb:  6 314 2022-11-29 11:31:47 reaching_go_spout_bar_nov22\n",
      "session nb:  7 314 2022-11-30 15:30:50 reaching_go_spout_bar_nov22\n",
      "session nb:  8 314 2022-12-01 11:17:17 reaching_go_spout_bar_nov22\n",
      "session nb:  9 314 2022-12-01 11:23:58 reaching_go_spout_bar_nov22\n",
      "session nb:  10 314 2022-12-02 13:48:39 reaching_go_spout_bar_nov22\n",
      "session nb:  11 314 2022-12-05 16:15:00 reaching_go_spout_bar_nov22\n",
      "session nb:  12 314 2022-12-06 16:14:17 reaching_go_spout_bar_nov22\n",
      "session nb:  13 314 2022-12-07 16:37:18 reaching_go_spout_bar_nov22\n",
      "session nb:  14 314 2022-12-08 18:35:56 reaching_go_spout_bar_nov22\n",
      "session nb:  15 314 2022-12-09 16:02:01 reaching_go_spout_bar_nov22\n",
      "session nb:  16 314 2022-12-12 15:49:43 reaching_go_spout_bar_nov22\n",
      "session nb:  17 314 2022-12-12 15:51:38 reaching_go_spout_bar_nov22\n",
      "session nb:  18 314 2022-12-13 14:09:58 reaching_go_spout_bar_nov22\n",
      "session nb:  19 314 2022-12-13 14:21:13 reaching_go_spout_bar_nov22\n",
      "session nb:  1 315 2022-11-21 14:08:26 reaching_go_spout_nov22\n",
      "session nb:  2 315 2022-11-21 14:10:42 reaching_go_spout_nov22\n",
      "session nb:  3 315 2022-11-21 14:37:03 reaching_go_spout_nov22\n",
      "session nb:  4 315 2022-11-21 15:00:45 reaching_go_spout_nov22\n",
      "session nb:  5 315 2022-11-25 11:35:47 reaching_go_spout_bar_nov22\n",
      "session nb:  6 315 2022-11-25 14:39:27 reaching_go_spout_bar_nov22\n",
      "session nb:  7 315 2022-11-26 14:15:54 reaching_go_spout_bar_nov22\n",
      "session nb:  8 315 2022-11-26 14:26:42 reaching_go_spout_bar_nov22\n",
      "session nb:  9 315 2022-11-28 15:20:59 reaching_go_spout_bar_nov22\n",
      "session nb:  10 315 2022-11-29 11:33:24 reaching_go_spout_bar_nov22\n",
      "session nb:  11 315 2022-11-30 15:28:46 reaching_go_spout_bar_nov22\n",
      "session nb:  12 315 2022-11-30 16:18:38 reaching_go_spout_bar_nov22\n",
      "session nb:  13 315 2022-12-01 12:18:14 reaching_go_spout_bar_nov22\n",
      "session nb:  14 315 2022-12-02 14:38:46 reaching_go_spout_bar_nov22\n",
      "session nb:  15 315 2022-12-02 14:39:12 reaching_go_spout_bar_nov22\n",
      "session nb:  16 315 2022-12-07 13:37:09 reaching_go_spout_bar_nov22\n",
      "session nb:  17 315 2022-12-07 13:44:48 reaching_go_spout_bar_nov22\n",
      "session nb:  18 315 2022-12-08 18:39:45 reaching_go_spout_bar_nov22\n",
      "session nb:  19 315 2022-12-08 18:59:57 reaching_go_spout_bar_nov22\n",
      "session nb:  20 315 2022-12-09 16:14:49 reaching_go_spout_bar_nov22\n",
      "session nb:  21 315 2022-12-10 18:21:36 reaching_go_spout_bar_nov22\n",
      "session nb:  22 315 2022-12-10 18:39:06 reaching_go_spout_bar_nov22\n",
      "session nb:  23 315 2022-12-12 14:48:41 reaching_go_spout_bar_nov22\n",
      "session nb:  24 315 2022-12-13 11:40:52 reaching_go_spout_bar_nov22\n",
      "session nb:  25 315 2022-12-14 14:35:25 reaching_go_spout_bar_nov22\n",
      "session nb:  1 316 2022-11-23 16:31:31 reaching_go_spout_bar_nov22\n",
      "session nb:  2 316 2022-11-23 16:32:25 reaching_go_spout_bar_nov22\n",
      "session nb:  3 316 2022-11-23 16:33:33 reaching_go_spout_bar_nov22\n",
      "session nb:  4 316 2022-11-23 16:34:30 reaching_go_spout_bar_nov22\n",
      "session nb:  5 316 2022-11-24 12:47:35 reaching_go_spout_bar_nov22\n",
      "session nb:  6 316 2022-11-25 12:12:46 reaching_go_spout_bar_nov22\n",
      "session nb:  7 316 2022-11-25 15:24:50 reaching_go_spout_bar_nov22\n",
      "session nb:  8 316 2022-11-26 14:22:55 reaching_go_spout_bar_nov22\n",
      "session nb:  9 316 2022-11-26 15:07:51 reaching_go_spout_bar_nov22\n",
      "session nb:  10 316 2022-11-28 16:08:32 reaching_go_spout_bar_nov22\n",
      "session nb:  11 316 2022-11-29 15:15:29 reaching_go_spout_bar_nov22\n",
      "session nb:  12 316 2022-11-30 16:34:44 reaching_go_spout_bar_nov22\n",
      "session nb:  13 316 2022-12-01 12:15:16 reaching_go_spout_bar_nov22\n",
      "session nb:  14 316 2022-12-02 14:34:27 reaching_go_spout_bar_nov22\n",
      "session nb:  15 316 2022-12-02 14:36:14 reaching_go_spout_bar_nov22\n",
      "session nb:  16 316 2022-12-05 16:19:11 reaching_go_spout_bar_nov22\n",
      "session nb:  17 316 2022-12-06 16:11:42 reaching_go_spout_bar_nov22\n",
      "session nb:  18 316 2022-12-06 16:55:04 reaching_go_spout_bar_nov22\n",
      "session nb:  1 317 2022-11-23 16:31:31 reaching_go_spout_bar_nov22\n",
      "session nb:  2 317 2022-11-23 16:32:25 reaching_go_spout_bar_nov22\n",
      "session nb:  3 317 2022-11-23 16:33:33 reaching_go_spout_bar_nov22\n",
      "session nb:  4 317 2022-11-23 16:34:30 reaching_go_spout_bar_nov22\n",
      "session nb:  5 317 2022-11-24 12:47:35 reaching_go_spout_bar_nov22\n",
      "session nb:  6 317 2022-11-25 12:12:46 reaching_go_spout_bar_nov22\n",
      "session nb:  7 317 2022-11-25 15:24:50 reaching_go_spout_bar_nov22\n",
      "session nb:  8 317 2022-11-26 14:22:55 reaching_go_spout_bar_nov22\n",
      "session nb:  9 317 2022-11-26 15:07:51 reaching_go_spout_bar_nov22\n",
      "session nb:  10 317 2022-11-28 16:08:32 reaching_go_spout_bar_nov22\n",
      "session nb:  11 317 2022-11-29 15:15:29 reaching_go_spout_bar_nov22\n",
      "session nb:  12 317 2022-11-30 17:54:22 reaching_go_spout_bar_nov22\n",
      "session nb:  13 317 2022-12-01 16:30:24 reaching_go_spout_bar_nov22\n",
      "session nb:  14 317 2022-12-02 16:06:06 reaching_go_spout_bar_nov22\n",
      "session nb:  15 317 2022-12-05 17:14:57 reaching_go_spout_bar_nov22\n",
      "session nb:  16 317 2022-12-05 17:23:59 reaching_go_spout_bar_nov22\n",
      "session nb:  17 317 2022-12-06 17:08:15 reaching_go_spout_bar_nov22\n",
      "session nb:  1 318 2022-11-21 16:19:06 reaching_go_spout_nov22\n",
      "session nb:  2 318 2022-11-23 16:42:40 reaching_go_spout_bar_nov22\n",
      "session nb:  3 318 2022-11-24 12:47:05 reaching_go_spout_bar_nov22\n",
      "session nb:  4 318 2022-11-25 12:14:52 reaching_go_spout_bar_nov22\n",
      "session nb:  5 318 2022-11-25 15:23:37 reaching_go_spout_bar_nov22\n",
      "session nb:  6 318 2022-11-28 16:10:44 reaching_go_spout_bar_nov22\n",
      "session nb:  7 318 2022-11-29 15:14:44 reaching_go_spout_bar_nov22\n",
      "session nb:  8 318 2022-11-29 15:26:25 reaching_go_spout_bar_nov22\n",
      "session nb:  9 318 2022-11-30 17:53:56 reaching_go_spout_bar_nov22\n",
      "session nb:  10 318 2022-12-01 16:32:32 reaching_go_spout_bar_nov22\n",
      "session nb:  11 318 2022-12-02 11:25:05 reaching_go_spout_bar_nov22\n",
      "session nb:  12 318 2022-12-02 11:28:39 reaching_go_spout_bar_nov22\n",
      "session nb:  13 318 2022-12-02 16:08:42 reaching_go_spout_bar_nov22\n",
      "session nb:  14 318 2022-12-02 16:16:16 reaching_go_spout_bar_nov22\n",
      "session nb:  15 318 2022-12-05 17:17:08 reaching_go_spout_bar_nov22\n",
      "session nb:  16 318 2022-12-06 17:11:08 reaching_go_spout_bar_nov22\n",
      "session nb:  17 318 2022-12-12 15:25:07 reaching_go_spout_bar_nov22\n",
      "session nb:  1 63 2023-02-02 11:28:09 reaching_go_spout_bar_nov22\n",
      "session nb:  2 63 2023-02-09 10:56:36 reaching_go_spout_bar_nov22\n",
      "session nb:  3 63 2023-02-10 15:29:38 reaching_go_spout_bar_nov22\n",
      "session nb:  4 63 2023-02-13 11:02:11 reaching_go_spout_bar_nov22\n",
      "session nb:  5 63 2023-02-14 12:20:51 reaching_go_spout_bar_nov22\n",
      "session nb:  6 63 2023-02-15 11:46:00 reaching_go_spout_bar_nov22\n",
      "session nb:  7 63 2023-02-16 12:30:32 reaching_go_spout_bar_nov22\n",
      "session nb:  8 63 2023-02-16 12:47:36 reaching_go_spout_bar_nov22\n",
      "session nb:  9 63 2023-02-17 11:24:45 reaching_go_spout_bar_nov22\n",
      "session nb:  10 63 2023-02-20 12:50:40 reaching_go_spout_bar_nov22\n",
      "session nb:  11 63 2023-02-20 12:56:21 reaching_go_spout_bar_nov22\n",
      "session nb:  12 63 2023-02-20 13:02:38 reaching_go_spout_bar_nov22\n",
      "session nb:  13 63 2023-02-20 13:23:06 reaching_go_spout_bar_nov22\n",
      "session nb:  14 63 2023-02-21 11:43:46 reaching_go_spout_bar_nov22\n",
      "session nb:  15 63 2023-02-22 16:18:37 reaching_go_spout_bar_nov22\n",
      "session nb:  16 63 2023-02-23 16:37:13 reaching_go_spout_bar_nov22\n",
      "session nb:  17 63 2023-02-23 17:20:43 reaching_go_spout_bar_nov22\n",
      "session nb:  18 63 2023-02-24 11:09:55 reaching_go_spout_bar_nov22\n",
      "session nb:  19 63 2023-02-27 16:44:26 reaching_go_spout_bar_nov22\n",
      "session nb:  20 63 2023-02-28 09:53:01 reaching_go_spout_bar_nov22\n",
      "session nb:  21 63 2023-02-28 10:22:14 reaching_go_spout_incr_break2_nov22\n",
      "session nb:  22 63 2023-03-01 10:31:45 reaching_go_spout_bar_nov22\n",
      "session nb:  23 63 2023-03-02 09:29:11 reaching_go_spout_bar_nov22\n",
      "session nb:  24 63 2023-03-03 09:58:36 reaching_go_spout_bar_nov22\n",
      "session nb:  25 63 2023-03-03 10:40:18 reaching_go_spout_bar_nov22\n",
      "session nb:  26 63 2023-03-06 10:42:02 reaching_go_spout_bar_nov22\n",
      "session nb:  27 63 2023-03-07 10:54:59 reaching_go_spout_bar_nov22\n",
      "session nb:  28 63 2023-03-09 11:09:21 reaching_go_spout_bar_nov22\n",
      "session nb:  29 63 2023-03-10 10:28:04 reaching_go_spout_bar_nov22\n",
      "session nb:  30 63 2023-03-13 16:42:28 reaching_go_spout_bar_mar23\n",
      "session nb:  31 63 2023-03-14 11:03:12 reaching_go_spout_bar_nov22\n",
      "session nb:  32 63 2023-03-15 13:12:55 reaching_go_spout_bar_mar23\n",
      "session nb:  33 63 2023-03-16 11:22:46 reaching_go_spout_bar_nov22\n",
      "session nb:  34 63 2023-03-16 11:32:27 reaching_go_spout_bar_nov22\n",
      "session nb:  35 63 2023-03-17 11:49:18 reaching_go_spout_bar_mar23\n",
      "session nb:  36 63 2023-03-20 15:11:14 reaching_go_spout_bar_mar23\n",
      "session nb:  37 63 2023-03-21 14:26:49 reaching_go_spout_bar_mar23\n",
      "session nb:  38 63 2023-03-22 18:33:59 reaching_go_spout_bar_nov22\n",
      "session nb:  39 63 2023-03-23 11:05:39 reaching_go_spout_bar_mar23\n",
      "session nb:  40 63 2023-03-23 21:23:44 reaching_go_spout_bar_mar23\n",
      "session nb:  41 63 2023-03-24 09:29:19 reaching_go_spout_bar_mar23\n",
      "session nb:  42 63 2023-03-24 17:29:04 reaching_go_spout_bar_mar23\n",
      "session nb:  43 63 2023-03-28 20:59:52 reaching_go_spout_bar_mar23\n",
      "session nb:  44 63 2023-03-29 19:45:17 reaching_go_spout_bar_mar23\n",
      "session nb:  45 63 2023-04-01 17:43:51 reaching_go_spout_bar_mar23\n",
      "session nb:  46 63 2023-04-01 18:09:30 reaching_go_spout_bar_mar23\n",
      "session nb:  47 63 2023-04-03 16:04:15 reaching_go_spout_bar_mar23\n",
      "session nb:  48 63 2023-04-04 19:19:24 reaching_go_spout_bar_mar23\n",
      "session nb:  49 63 2023-04-09 18:31:15 reaching_go_spout_bar_mar23\n",
      "session nb:  50 63 2023-04-10 19:43:31 reaching_go_spout_bar_mar23\n",
      "session nb:  51 63 2023-04-11 19:54:50 reaching_go_spout_bar_mar23\n",
      "session nb:  52 63 2023-04-12 20:25:02 reaching_go_spout_bar_mar23\n",
      "session nb:  53 63 2023-04-13 17:09:14 reaching_go_spout_bar_apr23\n",
      "session nb:  54 63 2023-04-14 16:01:12 reaching_go_spout_bar_apr23\n"
     ]
    }
   ],
   "source": [
    "# Load all raw text sessions in the indicated folder or a sessions.pkl file\n",
    "# if already existing in folder_path\n",
    "exp_cohort = Experiment(path=pycontrol_dir, int_subject_IDs=True, update=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp_cohort.sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 11, 19, 17, 58, 51)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_cohort.sessions[0].datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "exp_cohort.sessions = [session for session in exp_cohort.sessions if (session.subject_ID in [604, 602]) \\\n",
    "                       and (session.datetime.date() in [datetime.date(2023,3,20), datetime.date(2023,3,21), datetime.date(2023,3,22), datetime.date(2023,3,23), datetime.date(2023,3,24), datetime.date(2023,3,27)] )]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform extraction of behavioural information by trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trialexp.process.data_import.Session at 0x21dfe2b2640>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_cohort.sessions[0].get_session_by_trial(trial_window=trial_window, timelim=[0,1000], tasksfile=tasksfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing by trial: RE604-2023-03-20-094835.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE602-2023-03-20-145614.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE602-2023-03-21-143610.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE604-2023-03-21-154547.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE604-2023-03-22-120336.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE602-2023-03-22-121414.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE602-2023-03-22-124029.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE602-2023-03-23-085141.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE604-2023-03-23-090342.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE604-2023-03-24-091134.txt task: reaching_go_spout_bar_mar23\n",
      "processing by trial: RE604-2023-03-27-091207.txt task: reaching_go_spout_bar_mar23\n"
     ]
    }
   ],
   "source": [
    "# Process the whole experimental folder by trials\n",
    "\n",
    "exp_cohort.process_exp_by_trial(trial_window, timelim=None, tasksfile=tasksfile, verbose=True)\n",
    "\n",
    "# Save the file as sessions.pkl in folder_path\n",
    "# exp_cohort.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp_cohort.sessions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditions for delayed go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>break_after_abort</th>\n",
       "      <th>US_end_timer</th>\n",
       "      <th>water_on</th>\n",
       "      <th>water by bar_off</th>\n",
       "      <th>water by spout</th>\n",
       "      <th>water for free</th>\n",
       "      <th>water success</th>\n",
       "      <th>busy_win_timer</th>\n",
       "      <th>spout</th>\n",
       "      <th>button_press</th>\n",
       "      <th>waiting_for_spout</th>\n",
       "      <th>trigger</th>\n",
       "      <th>valid</th>\n",
       "      <th>success</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>hold_for_water</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>604_2023-03-20_09:48:35_455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     break_after_abort  US_end_timer  water_on  water by bar_off  \\\n",
       "1                 True         False     False             False   \n",
       "2                 True         False     False             False   \n",
       "3                 True         False     False             False   \n",
       "4                 True         False     False             False   \n",
       "5                False         False     False             False   \n",
       "..                 ...           ...       ...               ...   \n",
       "451               True         False     False             False   \n",
       "452               True         False     False             False   \n",
       "453               True         False     False             False   \n",
       "454               True         False     False             False   \n",
       "455               True          True      True              True   \n",
       "\n",
       "     water by spout  water for free  water success  busy_win_timer  spout  \\\n",
       "1             False           False          False           False   True   \n",
       "2             False           False          False           False   True   \n",
       "3             False           False          False           False   True   \n",
       "4             False           False          False           False   True   \n",
       "5             False           False          False           False  False   \n",
       "..              ...             ...            ...             ...    ...   \n",
       "451           False           False          False           False  False   \n",
       "452           False           False          False           False  False   \n",
       "453           False           False          False           False  False   \n",
       "454           False           False          False           False  False   \n",
       "455            True            True           True            True   True   \n",
       "\n",
       "     button_press  waiting_for_spout         trigger  valid  success  \\\n",
       "1           False              False  hold_for_water   True    False   \n",
       "2           False              False  hold_for_water   True    False   \n",
       "3           False              False  hold_for_water   True    False   \n",
       "4           False              False  hold_for_water   True    False   \n",
       "5           False               True  hold_for_water   True    False   \n",
       "..            ...                ...             ...    ...      ...   \n",
       "451         False              False  hold_for_water   True    False   \n",
       "452         False              False  hold_for_water   True    False   \n",
       "453         False              False  hold_for_water   True    False   \n",
       "454         False              False  hold_for_water   True    False   \n",
       "455          True               True  hold_for_water   True    False   \n",
       "\n",
       "                             uid  \n",
       "1      604_2023-03-20_09:48:35_1  \n",
       "2      604_2023-03-20_09:48:35_2  \n",
       "3      604_2023-03-20_09:48:35_3  \n",
       "4      604_2023-03-20_09:48:35_4  \n",
       "5      604_2023-03-20_09:48:35_5  \n",
       "..                           ...  \n",
       "451  604_2023-03-20_09:48:35_451  \n",
       "452  604_2023-03-20_09:48:35_452  \n",
       "453  604_2023-03-20_09:48:35_453  \n",
       "454  604_2023-03-20_09:48:35_454  \n",
       "455  604_2023-03-20_09:48:35_455  \n",
       "\n",
       "[455 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_cohort.sessions[0].df_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defime each trial type as a dictionary of conditions to be met\n",
    "conditions_dict1 = {'trigger': 'hold_for_water', 'success': True, 'water by spout': True}\n",
    "\n",
    "# conditions_dict2 = {'trigger': 'hold_for_water', 'success': False, 'hold_timer': True}\n",
    "\n",
    "conditions_dict2 = {'trigger': 'hold_for_water', 'success': False, 'break_after_abort': True}\n",
    "\n",
    "# conditions_dict2 = {'trigger': 'hold_for_water', 'spout':False, 'valid': True, 'busy_win_timer': False, 'button_press': False}\n",
    "\n",
    "# Aggregate all condition dictionaries in a list\n",
    "conditions_list = [conditions_dict1, conditions_dict2]\n",
    "# Aliases for conditions\n",
    "cond_aliases = ['Go - hold hit', 'Go - aborted']\n",
    "# Groups as a list of lists\n",
    "# groups = [[280, 281, 282, 289],[295, 282, 284, 285, 292, 297]]\n",
    "groups = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved \\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\pycontrol\\reaching_go_spout_bar_nov22\\sessions.pkl\n"
     ]
    }
   ],
   "source": [
    "# Find if there is a matching photometry file:\n",
    "exp_cohort.match_sessions_to_files(photometry_dir, ext='ppd')\n",
    "\n",
    "# rsync synchronization pulses matching between behaviour and photometry\n",
    "exp_cohort.sync_photometry_files(2)\n",
    "exp_cohort.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed_mixed_mixed = exp_cohort.behav_events_to_dataset(\n",
    "        groups = None,\n",
    "        conditions_list = conditions_list, \n",
    "        cond_aliases = cond_aliases, \n",
    "        when = 'all', \n",
    "        task_names = ['reaching_go_spout_bar_nov22'], #'reaching_go_spout_incr_break2_nov22' ], #'reaching_go_spout_bar_dual_all_reward_dec22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "        trig_on_ev = None\n",
    "        )\n",
    "\n",
    "ev_dataset_mixed_mixed_mixed.set_trial_window(trial_window=[-4000,6000], unit='milliseconds')\n",
    "# ev_dataset.set_conditions(conditions=condition_list, aliases=cond_aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['keep', 'trial_ID'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(ev_dataset_mixed_mixed_mixed.metadata_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'session_nb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index_by_session \u001b[39m=\u001b[39m ev_dataset_mixed_mixed_mixed\u001b[39m.\u001b[39;49mmetadata_df\u001b[39m.\u001b[39;49msort_values(by\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39msession_nb\u001b[39;49m\u001b[39m'\u001b[39;49m], ascending\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39mindex\n\u001b[0;32m      3\u001b[0m ev_dataset_mixed_mixed_mixed\u001b[39m.\u001b[39mmetadata_df \u001b[39m=\u001b[39m ev_dataset\u001b[39m.\u001b[39mmetadata_df\u001b[39m.\u001b[39miloc[index_by_session]\n\u001b[0;32m      4\u001b[0m ev_dataset_mixed_mixed_mixed\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m ev_dataset\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39miloc[index_by_session]\n",
      "File \u001b[1;32mc:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\pandas\\core\\frame.py:6912\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6908\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(by):\n\u001b[0;32m   6909\u001b[0m     \u001b[39m# len(by) == 1\u001b[39;00m\n\u001b[0;32m   6911\u001b[0m     by \u001b[39m=\u001b[39m by[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 6912\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label_or_level_values(by, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   6914\u001b[0m     \u001b[39m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[0;32m   6915\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   6916\u001b[0m         \u001b[39m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[0;32m   6917\u001b[0m         \u001b[39m# \"Series\", variable has type \"ndarray\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ndcn1349\\Anaconda3\\envs\\trialexp\\lib\\site-packages\\pandas\\core\\generic.py:1850\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     values \u001b[39m=\u001b[39m (\n\u001b[0;32m   1845\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\n\u001b[0;32m   1846\u001b[0m         \u001b[39m.\u001b[39mget_level_values(key)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m         \u001b[39m.\u001b[39m_values\n\u001b[0;32m   1848\u001b[0m     )\n\u001b[0;32m   1849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1850\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1852\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'session_nb'"
     ]
    }
   ],
   "source": [
    "index_by_session = ev_dataset_mixed_mixed_mixed.metadata_df.sort_values(by=['session_nb'], ascending=False).index\n",
    "\n",
    "ev_dataset_mixed_mixed_mixed.metadata_df = ev_dataset.metadata_df.iloc[index_by_session]\n",
    "ev_dataset_mixed_mixed_mixed.data = ev_dataset.data.iloc[index_by_session]\n",
    "\n",
    "ev_dataset_mixed_mixed_mixed.filter_reset()\n",
    "ev_dataset_mixed_mixed_mixed.filter_lastNsessions(3)\n",
    "ev_dataset_mixed_mixed_mixed.filterout_subjects([602,604])\n",
    "\n",
    "ev_dataset_mixed_mixed_mixed.data.loc[:,'first_bar_off_trial_time'] = ev_dataset.data['bar_off_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed_mixed_mixed.data.loc[:,'first_spout_trial_time'] = ev_dataset.data['spout_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed_mixed_mixed.plot_raster(keys=['first_bar_off_trial_time','first_spout_trial_time'], module='matplotlib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ev_dataset.plot_raster(keys=['first_bar_off_trial_time', 'first_spout_trial_time'], module='matplotlib')\n",
    "ev_dataset_nocond = deepcopy(ev_dataset)\n",
    "ev_dataset_nocond.filter_min_by_session(10)\n",
    "ev_dataset_nocond.filterout_subjects([602,604])\n",
    "ev_dataset_nocond.filter_lastNsessions(10)\n",
    "ev_dataset_nocond.plot_raster(keys=['first_bar_off_trial_time', 'first_spout_trial_time'], module='matplotlib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaviour: Compute distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset.filter_reset()\n",
    "ev_dataset.filter_min_by_session(20)\n",
    "ev_dataset.filterout_subjects([58,61,63,313,314,315])\n",
    "# ev_dataset.filter_lastNsessions(3)\n",
    "\n",
    "\n",
    "dist_as_continuous = ev_dataset.compute_distribution(\n",
    "        trial_window = [-1999, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "# dist_as_continuous.set_conditions(conditions=conditions_list, aliases=cond_aliases)\n",
    "# Remove test files\n",
    "# dist_as_continuous.filterout_subjects([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_as_continuous.heatmap(\n",
    "    vars = ['first_bar_off_dist', 'first_spout_dist'],\n",
    "    time_lim = [-1000, 6000],\n",
    "    colormap = 'jet'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaviour: Plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_as_continuous.filter_reset()\n",
    "ev_dataset.filter_min_by_session(20)\n",
    "ev_dataset.filterout_subjects([58,61,63,313,314,315])\n",
    "ev_dataset.filter_lastNsessions(3)\n",
    "\n",
    "dist_as_continuous = ev_dataset.compute_distribution(\n",
    "        trial_window = [-1999, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "\n",
    "dist_as_continuous.lineplot(\n",
    "    vars = [ 'first_bar_off_dist','first_spout_dist'],\n",
    "    time_lim = [-1000,6000],\n",
    "    error = True,\n",
    "    ylim = None,#[[-0.1,1.6]], #[[-0.1, 0.7]], #[[-0.1, 1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "    colormap = 'jet',\n",
    "    legend = True,\n",
    "    plot_subjects = True,\n",
    "    plot_groups = True,\n",
    "    figsize = (15,10),\n",
    "    dpi = 100,\n",
    "    verbose = False)\n",
    "# Return a count of overall number of trials\n",
    "dist_as_continuous.metadata_df['keep'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match and synchronize photometry to behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if there is a matching photometry file:\n",
    "exp_cohort.match_sessions_to_files(photometry_dir, ext='ppd')\n",
    "\n",
    "# rsync synchronization pulses matching between behaviour and photometry\n",
    "exp_cohort.sync_photometry_files(2)\n",
    "exp_cohort.save()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop to plot photometry trials triggered on different events\n",
    "- 1: Trial onset (CS-onset = hold period start)\n",
    "- 2: First bar_off\n",
    "- 3: Last bar_off before spout\n",
    "- 4: First spout\n",
    "- 5: Reward (US_end_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigs = [None, 'bar_off', 'bar_off', 'spout', 'US_end_timer']\n",
    "last_befores = [None, None, 'spout', None, None]\n",
    "\n",
    "photo_dataset = dict()\n",
    "for idx, trig in enumerate(trigs):\n",
    "    photo_dataset[idx] = exp_cohort.get_photometry_groups(\n",
    "            groups = None, # or use groups variable defined above\n",
    "            conditions_list = conditions_list, \n",
    "            cond_aliases = cond_aliases,\n",
    "            trial_window = trial_window,\n",
    "            when = 'all', \n",
    "            task_names = ['reaching_go_spout_bar_nov22'],\n",
    "            trig_on_ev = trig, # align to the first event of a kind e.g. bar_off\n",
    "            last_before = last_befores[idx],\n",
    "            high_pass = None, \n",
    "            low_pass = 45, \n",
    "            median_filt = 3,\n",
    "            motion_corr = True, \n",
    "            df_over_f = True,\n",
    "            z_score = True, \n",
    "            downsampling_factor = 10, \n",
    "            export_vars = ['analog_1_df_over_f', 'zscored_df_over_f'], \n",
    "            # remove_artifacts = False, # To Deprecate in favor of Exp level artifact clustering\n",
    "            verbose = True) # will plot all the process of remove_artifacts if remove_artifacts == True\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot photometry trials triggered on different events based on above extraction\n",
    "- 1: Trial onset (CS-onset = hold period start)\n",
    "- 2: First bar_off\n",
    "- 3: Last bar_off before spout\n",
    "- 4: First spout\n",
    "- 5: Reward (US_end_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, trig in enumerate(trigs):\n",
    "\n",
    "    # keep only 316\n",
    "    photo_dataset[idx].filter_reset()\n",
    "    photo_dataset[idx].filterout_subjects([0,1,58,61,63,313,314,315,318])\n",
    "    photo_dataset[idx].filter_min_by_session(min_trials = 10)\n",
    "    photo_dataset[idx].filter_lastNsessions(n = 3)\n",
    "    if idx == 4:\n",
    "        photo_dataset[idx].filterout_conditions(1)\n",
    "    photo_dataset[idx].lineplot(\n",
    "        vars = ['zscored_df_over_f'],\n",
    "        time_lim = [-3000, 6000],\n",
    "        # time_unit = 'seconds',\n",
    "        ylim = [[-1, 5]],# [[-0.004, 0.006]],#[[-0.03, 0.1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "        error = True,\n",
    "        colormap = 'jet',\n",
    "        legend = True,\n",
    "        plot_subjects = True,\n",
    "        plot_groups = True,\n",
    "        liney0 = False,\n",
    "        linex0 = True,\n",
    "        figsize = (15, 5),\n",
    "        dpi = 100,\n",
    "        verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same plot, heatmap version:\n",
    "\n",
    "- 1: Trial onset (CS-onset = hold period start)\n",
    "- 2: First bar_off\n",
    "- 3: Last bar_off before spout\n",
    "- 4: First spout\n",
    "- 5: Reward (US_end_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_labels = ('Cue_onset','First_mov','Mov_bef_spout','Spout','Reward')\n",
    "\n",
    "for idx, trig in enumerate(trigs):\n",
    "    photo_dataset[idx].filter_reset()\n",
    "    photo_dataset[idx].filterout_subjects([0,1,61,63,313,314,315,317,318])\n",
    "    photo_dataset[idx].filterout_dates([datetime(2023,2,24).date(), datetime(2023,2,23).date()])\n",
    "    photo_dataset[idx].filter_lastNsessions(n = 1)\n",
    "    fig = photo_dataset[idx].heatmap(\n",
    "        vars = ['zscored_df_over_f'],\n",
    "        time_lim = [-3000, 6000],\n",
    "        clim_pctile = None,\n",
    "        colormap = 'jet'\n",
    "    )   \n",
    "    photo_dataset[idx].filter_reset()\n",
    "\n",
    "\n",
    "    file_path = 'C:\\\\Users\\\\phar0732\\\\Documents\\\\GitHub\\\\trialexp\\\\outputs\\\\' + 'photo_heatmap_' + phase_labels[idx] + '.pdf'\n",
    "    fig.savefig(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_dataset[idx].metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From julien-pc\n",
    "photometry_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\_Other\\test_folder\\delayed_go\\pyphotometry\\delayed_go_dual_2022'\n",
    "pycontrol_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\_Other\\test_folder\\delayed_go\\pycontrol\\delayed_go_dual_2022'\n",
    "trial_window=[-3000,6000]\n",
    "exp_cohort_mixed = Experiment(path=pycontrol_dir, int_subject_IDs=True, update=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cohort_mixed.process_exp_by_trial(trial_window, timelim=None, tasksfile=tasksfile, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defime each trial type as a dictionary of conditions to be met\n",
    "conditions_dict1 = {'trigger': 'hold_for_water', 'success': True, 'water by spout': True}\n",
    "\n",
    "# conditions_dict2 = {'trigger': 'hold_for_water', 'success': False, 'hold_timer': True}\n",
    "\n",
    "conditions_dict2 = {'trigger': 'hold_for_water', 'success': False, 'break_after_abort': True}\n",
    "\n",
    "# conditions_dict2 = {'trigger': 'hold_for_water', 'spout':False, 'valid': True, 'busy_win_timer': False, 'button_press': False}\n",
    "\n",
    "# Aggregate all condition dictionaries in a list\n",
    "conditions_list = [conditions_dict1, conditions_dict2]\n",
    "# Aliases for conditions\n",
    "cond_aliases = ['Go - hold hit', 'Go - aborted']\n",
    "# Groups as a list of lists\n",
    "# groups = [[280, 281, 282, 289],[295, 282, 284, 285, 292, 297]]\n",
    "groups = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed = exp_cohort_mixed.behav_events_to_dataset(\n",
    "        groups = None,\n",
    "        conditions_list = conditions_list, \n",
    "        cond_aliases = cond_aliases, \n",
    "        when = 'all', \n",
    "        task_names = ['reaching_go_spout_bar_nov22' ,'reaching_go_spout_bar_dual_dec22','reaching_go_spout_bar_all_reward_dec22','reaching_go_spout_bar_all_reward_dec22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "        trig_on_ev = None)\n",
    "\n",
    "ev_dataset_mixed.set_trial_window(trial_window=[-3000,6000], unit='milliseconds')\n",
    "# ev_dataset.set_conditions(conditions=condition_list, aliases=cond_aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_by_session = ev_dataset_mixed.metadata_df.sort_values(by=['session_nb'], ascending=False).index\n",
    "\n",
    "ev_dataset_mixed.metadata_df = ev_dataset_mixed.metadata_df.iloc[index_by_session]\n",
    "ev_dataset_mixed.data = ev_dataset_mixed.data.iloc[index_by_session]\n",
    "\n",
    "ev_dataset_mixed.filter_reset()\n",
    "ev_dataset_mixed.filter_lastNsessions(5)\n",
    "ev_dataset_mixed.filterout_subjects([58,61,63,313,314,315])\n",
    "\n",
    "ev_dataset_mixed.data.loc[:,'first_bar_off_trial_time'] = ev_dataset_mixed.data['bar_off_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed.data.loc[:,'first_spout_trial_time'] = ev_dataset_mixed.data['spout_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed.plot_raster(keys=['first_bar_off_trial_time','first_spout_trial_time'], module='matplotlib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed.filter_reset()\n",
    "ev_dataset_mixed.filter_min_by_session(20)\n",
    "ev_dataset_mixed.filterout_subjects([313,314,315])\n",
    "# ev_dataset.filter_lastNsessions(3)\n",
    "\n",
    "\n",
    "dist_as_continuous_mixed = ev_dataset_mixed.compute_distribution(\n",
    "        trial_window = [-3000, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "# dist_as_continuous.set_conditions(conditions=conditions_list, aliases=cond_aliases)\n",
    "# Remove test files\n",
    "# dist_as_continuous.filterout_subjects([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed_no_cond = exp_cohort_mixed.behav_events_to_dataset(\n",
    "        groups = None,\n",
    "        conditions_list = None, \n",
    "        cond_aliases = None, \n",
    "        when = 'all', \n",
    "        task_names = ['reaching_go_spout_bar_nov22' ,'reaching_go_spout_bar_dual_dec22','reaching_go_spout_bar_all_reward_dec22','reaching_go_spout_bar_all_reward_dec22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "        trig_on_ev = None)\n",
    "\n",
    "ev_dataset_mixed_no_cond.set_trial_window(trial_window=[-2000,6000], unit='milliseconds')\n",
    "# ev_dataset.set_con\n",
    "\n",
    "index_by_session = ev_dataset_mixed_no_cond.metadata_df.sort_values(by=['session_nb'], ascending=True).index\n",
    "\n",
    "ev_dataset_mixed_no_cond.metadata_df = ev_dataset_mixed_no_cond.metadata_df.iloc[index_by_session]\n",
    "ev_dataset_mixed_no_cond.data = ev_dataset_mixed_no_cond.data.iloc[index_by_session]\n",
    "\n",
    "ev_dataset_mixed_no_cond.filter_reset()\n",
    "# ev_dataset_mixed_no_cond.filter_lastNsessions(10)\n",
    "ev_dataset_mixed_no_cond.filterout_subjects([58,61,63,313,314,315])\n",
    "\n",
    "ev_dataset_mixed_no_cond.data.loc[:,'first_bar_off_trial_time'] = ev_dataset_mixed_no_cond.data['bar_off_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed_no_cond.data.loc[:,'first_spout_trial_time'] = ev_dataset_mixed_no_cond.data['spout_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed_no_cond.plot_raster(keys=['first_bar_off_trial_time','first_spout_trial_time'], module='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed_no_cond.filter_reset()\n",
    "# ev_dataset_mixed_no_cond.filter_lastNsessions(10)\n",
    "ev_dataset_mixed_no_cond.filter_min_by_session(40)\n",
    "ev_dataset_mixed_no_cond.filterout_subjects([313,314,315])\n",
    "\n",
    "dist_as_continuous_mixed_no_cond = ev_dataset_mixed_no_cond.compute_distribution(\n",
    "        trial_window = [-2000, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "\n",
    "dist_as_continuous_mixed_no_cond.heatmap(\n",
    "    vars = ['bar_off_dist', 'spout_dist'],\n",
    "    time_lim = [-2000, 6000],\n",
    "    colormap = 'jet',\n",
    "    figsize = (7, 5),\n",
    "    dpi = 80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed_no_cond.filter_reset()\n",
    "ev_dataset_mixed_no_cond.filter_min_by_session(40)\n",
    "ev_dataset_mixed_no_cond.filterout_subjects([58,61,63,313,314,315])\n",
    "\n",
    "# ev_dataset_mixed_no_cond.filter_lastNdays(3)\n",
    "ev_dataset_mixed_no_cond.filter_firstNsessions(3)\n",
    "\n",
    "dist_as_continuous_mixed_no_cond = ev_dataset_mixed_no_cond.compute_distribution(\n",
    "        trial_window = [-2000, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "\n",
    "fig, axs, out_df = dist_as_continuous_mixed_no_cond.lineplot(\n",
    "    vars = [ 'first_bar_off_dist','first_spout_dist'],\n",
    "    time_lim = [-2000,6000],\n",
    "    error = True,\n",
    "    ylim =[[-0.0,1.5],[-0.0,1]], #[[-0.1, 0.7]], #[[-0.1, 1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "    colormap = 'jet',\n",
    "    legend = False,\n",
    "    plot_subjects = True,\n",
    "    plot_groups = True,\n",
    "    figsize = (7,5),\n",
    "    dpi = 80,\n",
    "    verbose = False)\n",
    "\n",
    "fig.savefig(r'C:\\Users\\phar0732\\Documents\\GitHub\\trialexp\\outputs\\first_ev_dist_first3s.pdf')\n",
    "\n",
    "fig = ev_dataset_mixed_no_cond.plot_raster(\n",
    "    keys=['first_bar_off_trial_time','first_spout_trial_time'], \n",
    "    module='matplotlib',\n",
    "    figsize=(3.25,5))\n",
    "\n",
    "fig.savefig(r'C:\\Users\\phar0732\\Documents\\GitHub\\trialexp\\outputs\\first_ev_raster_first3s.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_as_continuous_mixed.heatmap(\n",
    "    vars = ['first_bar_off_dist', 'first_spout_dist'],\n",
    "    time_lim = [-1000, 6000],\n",
    "    colormap = 'jet'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaviour: Plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_as_continuous_mixed.filter_reset()\n",
    "ev_dataset_mixed.filter_min_by_session(10)\n",
    "ev_dataset_mixed.filterout_subjects([58,61,63,313,314,315])\n",
    "ev_dataset_mixed.filter_lastNsessions(3)\n",
    "\n",
    "dist_as_continuous_mixed = ev_dataset_mixed.compute_distribution(\n",
    "        trial_window = [-3000, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "\n",
    "dist_as_continuous_mixed.lineplot(\n",
    "    vars = [ 'first_bar_off_dist','first_spout_dist'],\n",
    "    time_lim = [-1000,6000],\n",
    "    error = True,\n",
    "    ylim = None,#[[-0.1,1.6]], #[[-0.1, 0.7]], #[[-0.1, 1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "    colormap = 'jet',\n",
    "    legend = True,\n",
    "    plot_subjects = True,\n",
    "    plot_groups = True,\n",
    "    figsize = (15,10),\n",
    "    dpi = 100,\n",
    "    verbose = False)\n",
    "# Return a count of overall number of trials\n",
    "dist_as_continuous_mixed.metadata_df['keep'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if there is a matching photometry file:\n",
    "exp_cohort_mixed.match_sessions_to_files(photometry_dir, ext='ppd')\n",
    "\n",
    "# rsync synchronization pulses matching between behaviour and photometry\n",
    "exp_cohort_mixed.sync_photometry_files(2)\n",
    "exp_cohort_mixed.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigs = [None, 'bar_off', 'bar_off', 'spout', 'US_end_timer']\n",
    "last_befores = [None, None, 'spout', None, None]\n",
    "trial_window = [-4000, 6000]\n",
    "photo_dataset_mixed = dict()\n",
    "for idx, trig in enumerate(trigs):\n",
    "    photo_dataset_mixed[idx] = exp_cohort_mixed.get_photometry_groups(\n",
    "            groups = None, # or use groups variable defined above\n",
    "            conditions_list = conditions_list, \n",
    "            cond_aliases = cond_aliases,\n",
    "            trial_window = trial_window,\n",
    "            when = 'all', \n",
    "            task_names = ['reaching_go_spout_bar_nov22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "            trig_on_ev = trig, # align to the first event of a kind e.g. bar_off\n",
    "            last_before = last_befores[idx],\n",
    "            high_pass = 0.01, \n",
    "            low_pass = 45, \n",
    "            median_filt = 3,\n",
    "            motion_corr = True, \n",
    "            df_over_f = True,\n",
    "            z_score = True, \n",
    "            downsampling_factor = 10, \n",
    "            export_vars = ['analog_1_df_over_f', 'zscored_df_over_f'], \n",
    "            # remove_artifacts = False, # To Deprecate in favor of Exp level artifact clustering\n",
    "            verbose = True) # will plot all the process of remove_artifacts if remove_artifacts == True\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot photometry trials triggered on different events based on above extraction\n",
    "- 1: Trial onset (CS-onset = hold period start)\n",
    "- 2: First bar_off\n",
    "- 3: Last bar_off before spout\n",
    "- 4: First spout\n",
    "- 5: Reward (US_end_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_labels = ('Cue_onset','First_mov','Mov_bef_spout','Spout','Reward')\n",
    "\n",
    "for idx, trig in enumerate(trigs):\n",
    "\n",
    "    photo_dataset_mixed[idx].filter_reset()\n",
    "    photo_dataset_mixed[idx].filterout_subjects([0,1,58,63,313,314,315,318])\n",
    "    photo_dataset_mixed[idx].filter_min_by_session(min_trials = 10)\n",
    "    photo_dataset_mixed[idx].filter_lastNdays(n = 3)\n",
    "    if idx == 4:\n",
    "        figsize = (9.75, 5)\n",
    "    else:\n",
    "        figsize = (15, 5)\n",
    "\n",
    "    fig, axs, out_df = photo_dataset_mixed[idx].lineplot(\n",
    "        vars = ['zscored_df_over_f'],\n",
    "        time_lim = [-500, 500],\n",
    "        # time_unit = 'seconds',\n",
    "        ylim = [[-1, 5]],# [[-0.004, 0.006]],#[[-0.03, 0.1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "        error = True,\n",
    "        colormap = 'jet',\n",
    "        legend = True,\n",
    "        plot_subjects = True,\n",
    "        plot_groups = True,\n",
    "        liney0 = False,\n",
    "        linex0 = True,\n",
    "        figsize = figsize,\n",
    "        dpi = 100,\n",
    "        verbose = False)\n",
    "\n",
    "    file_path = 'C:\\\\Users\\\\phar0732\\\\Documents\\\\GitHub\\\\trialexp\\\\outputs\\\\' + 'photo_ave_' + phase_labels[idx] + '.pdf'\n",
    "    fig.savefig(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, trig in enumerate(trigs):\n",
    "#     photo_dataset_mixed[idx].filter_reset()\n",
    "#     photo_dataset_mixed[idx].filterout_subjects([0,1,313,314,315,318])\n",
    "#     photo_dataset_mixed[idx].filter_min_by_session(min_trials = 30)\n",
    "#     photo_dataset_mixed[idx].filter_lastNsessions(n = 3)\n",
    "#     photo_dataset_mixed[idx].heatmap(\n",
    "#         vars = ['zscored_df_over_f'],\n",
    "#         time_lim = [-1000, 1000],\n",
    "#         clim_pctile = None,\n",
    "#         colormap = 'jet'\n",
    "#     )   \n",
    "#     photo_dataset_mixed[idx].filter_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.loc[:,'date'] = metadata_df['datetime'].apply(lambda d: d.date())\n",
    "dates = metadata_df.loc[:,'date'].unique()\n",
    "dates_norm = np.linspace(0,1,len(dates))\n",
    "dict(zip(dates,dates_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trial_nb_by_day(row, max_trial_nb_by_day):\n",
    "    '''\n",
    "    Helper function to use with apply() on a metadata_df DataFrame.\n",
    "    It re-computes the trial_nb based on total trial in a day in case\n",
    "    of multiple sessions in one day.\n",
    "    '''\n",
    "    # list sessions numbers for the day for this subject\n",
    "\n",
    "    # inefficient to put here, slow down computation\n",
    "    sessions_nb = max_trial_nb_by_day.loc[\n",
    "        (row.subject_ID, row.date, max_trial_nb_by_day.index.get_level_values(2)),:].index.get_level_values(2).values\n",
    "    \n",
    "    sessions_nb = list(sessions_nb)\n",
    "\n",
    "    if row.session_nb == sessions_nb[0]:\n",
    "        # print('row == session_nb')\n",
    "        return row.trial_nb\n",
    "    else:\n",
    "\n",
    "        # return the number of trials before this session on the day\n",
    "        prev_trial_nb = max_trial_nb_by_day.loc[(row.subject_ID, row.date, sessions_nb[:sessions_nb.index(row.session_nb)]),'trial_nb'].cumsum().values[-1]\n",
    "        # print(prev_trial_nb)\n",
    "        return row.trial_nb + prev_trial_nb\n",
    "\n",
    "\n",
    "def trial_nb_normalization(metadata_df, by_day: bool = False):\n",
    "    '''\n",
    "    Compute trial_nb normalized position by session, or by day (if by_day = True) \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        metadata_df: pd.DataFrame\n",
    "            The DataFrame containing metadata for each trial of the trial_dataset\n",
    "        by_day: bool\n",
    "            If True, aggregates trials from sessions performed on the same day to\n",
    "            compute their normalized position\n",
    "    Return:\n",
    "    -------\n",
    "        metadata_df: pd.DataFrame\n",
    "\n",
    "    '''\n",
    "\n",
    "    if by_day:\n",
    "        metadata_df.loc[:,'date'] = metadata_df['datetime'].apply(lambda d: d.date())\n",
    "        max_trial_nb_by_day = metadata_df.groupby(['subject_ID', 'date', 'session_nb']).agg({'trial_nb':'max'})\n",
    "\n",
    "        metadata_df.loc[:,'trial_nb_day'] = metadata_df.apply(lambda x: compute_trial_nb_by_day(x, max_trial_nb_by_day), axis=1)\n",
    "       \n",
    "        max_trial_nb_by_day = max_trial_nb_by_day.groupby(['subject_ID', 'date']).agg({'trial_nb':'sum'})\n",
    "\n",
    "        metadata_df.loc[:,'trial_nb_norm'] = metadata_df.apply(lambda x: x.trial_nb_day / max_trial_nb_by_day.loc[(x['subject_ID'], x['date'])], axis=1)\n",
    "\n",
    "    else:\n",
    "        max_trial_nb = metadata_df.groupby(['subject_ID','session_nb']).agg({'trial_nb':'max'})\n",
    "        metadata_df.loc[:,'trial_nb_norm'] = metadata_df.apply(lambda x: x.trial_nb / max_trial_nb.loc[(x['subject_ID'], x['session_nb'])], axis=1)\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "def session_nb_normalization(metadata_df, by_day: bool = False):\n",
    "    '''\n",
    "    Compute session_nb normalized position by session, or by day (if by_day = True) \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        metadata_df: pd.DataFrame\n",
    "            The DataFrame containing metadata for each trial of the trial_dataset\n",
    "        by_day: bool\n",
    "            If True, aggregates sessions performed on the same day to\n",
    "            compute their normalized position\n",
    "    Return:\n",
    "    -------\n",
    "        metadata_df: pd.DataFrame\n",
    "\n",
    "    '''\n",
    "    metadata_df.loc[:,'date'] = metadata_df['datetime'].apply(lambda d: d.date())\n",
    "\n",
    "    if by_day:\n",
    "        dates_norm_dict = dict()\n",
    "        for subject_ID in metadata_df.subject_ID.unique():\n",
    "\n",
    "            dates = metadata_df.loc[metadata_df.subject_ID == subject_ID,'date'].unique()\n",
    "            dates_norm = np.linspace(0,1,len(dates))\n",
    "            dates_norm_dict[subject_ID] = dict(zip(dates,dates_norm))\n",
    "\n",
    "        metadata_df.loc[:,'session_nb_norm'] = metadata_df.apply(lambda x: dates_norm_dict[x.subject_ID][x.date], axis=1)\n",
    "\n",
    "    else:\n",
    "        dates_norm_dict = dict()\n",
    "        for subject_ID in metadata_df.subject_ID.unique():\n",
    "\n",
    "            sessions = metadata_df.loc[metadata_df.subject_ID == subject_ID, 'session_nb'].unique()\n",
    "            sessions_norm = np.linspace(0,1,len(sessions))\n",
    "            dates_norm_dict[subject_ID] = dict(zip(sessions,sessions_norm))\n",
    "\n",
    "        metadata_df.loc[:,'session_nb_norm'] = metadata_df.apply(lambda x: dates_norm_dict[x.subject_ID][x.session_nb], axis=1)\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "# def trial_nb_normalization_by_day(metadata_df):\n",
    "\n",
    "#     max_trial_nb = metadata_df.groupby(['subject_ID','session_nb']).agg({'trial_nb':'max'})\n",
    "#     metadata_df['trial_nb_norm'] = metadata_df.apply(lambda x: x.trial_nb / max_trial_nb.loc[(x['subject_ID'], x['session_nb'])], axis=1)\n",
    "#     return metadata_df\n",
    "\n",
    "def trial_nb_quantilization(metadata_df: pd.DataFrame, quantiles: tuple):\n",
    "    '''\n",
    "    Assign quantile index to each trial based on the trial_nb_norm value\n",
    "    computed by the trial_nb_normalization(metadata_df) method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        metadata_df: pd.DataFrame\n",
    "            The DataFrame containing metadata for each trial of the dataset\n",
    "        quantiles: tuple\n",
    "            A tuple of 2 entries tuples indicating the limits of the desired quantiles.\n",
    "            The lower limit is not included while the upper limit is.\n",
    "  \n",
    "        example:\n",
    "\n",
    "        >> metadata_df = trial_nb_normalization(metadata_df)\n",
    "        # Break down the trial numbers into 3 thirds\n",
    "        >> quantiles = ((0,0.33),(0.33,0.66),(0.66,1))\n",
    "        >> metadata_df = trial_nb_quantilization(metadata_df, quantiles = quantiles)\n",
    "\n",
    "    \n",
    "    Return the same DataFrame with the correspondign quantile indices\n",
    "    '''\n",
    "\n",
    "    if 'trial_nb_norm' not in metadata_df.columns:\n",
    "        raise Exception('You need to compute first trial_nb_normalization(metadata_df)')\n",
    "\n",
    "    metadata_df['trial_nb_quantile'] = nan\n",
    "    for q_nb, quantile_lim in enumerate(quantiles):\n",
    "        metadata_df.loc[((metadata_df['trial_nb_norm'] > quantile_lim[0]) & (metadata_df['trial_nb_norm'] <= quantile_lim[1])) , 'trial_nb_quantile'] = q_nb\n",
    "\n",
    "    metadata_df['trial_nb_quantile'] = metadata_df['trial_nb_quantile'].astype(int)\n",
    "\n",
    "    return metadata_df\n",
    "\n",
    "def session_nb_quantilization(metadata_df: pd.DataFrame, quantiles: tuple):\n",
    "    '''\n",
    "    Assign quantile index to each trial based on the trial_nb_norm value\n",
    "    computed by the trial_nb_normalization(metadata_df) method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        metadata_df: pd.DataFrame\n",
    "            The DataFrame containing metadata for each trial of the dataset\n",
    "        quantiles: tuple\n",
    "            A tuple of 2 entries tuples indicating the limits of the desired quantiles.\n",
    "            The lower limit is not included while the upper limit is.\n",
    "  \n",
    "        example:\n",
    "\n",
    "        >> metadata_df = trial_nb_normalization(metadata_df)\n",
    "        # Break down the trial numbers into 3 thirds\n",
    "        >> quantiles = ((0,0.33),(0.33,0.66),(0.66,1))\n",
    "        >> metadata_df = trial_nb_quantilization(metadata_df, quantiles = quantiles)\n",
    "\n",
    "    \n",
    "    Return the same DataFrame with the correspondign quantile indices\n",
    "    '''\n",
    "\n",
    "    if 'session_nb_norm' not in metadata_df.columns:\n",
    "        raise Exception('You need to compute first session_nb_normalization(metadata_df)')\n",
    "\n",
    "    metadata_df['session_nb_quantile'] = nan\n",
    "    for q_nb, quantile_lim in enumerate(quantiles):\n",
    "        metadata_df.loc[((metadata_df['session_nb_norm'] >= quantile_lim[0]) & (metadata_df['session_nb_norm'] <= quantile_lim[1])) , 'session_nb_quantile'] = q_nb\n",
    "\n",
    "    metadata_df['session_nb_quantile'] = metadata_df['session_nb_quantile'].astype(int)\n",
    "\n",
    "    return metadata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "trial_dataset_dict = dict()\n",
    "for d_idx in photo_dataset_mixed.keys():\n",
    "    trial_dataset_dict[d_idx] = deepcopy(photo_dataset_mixed[d_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "photo_var = 'zscored_df_over_f'\n",
    "# quantiles = ((0,0.33),(0.33,0.66),(0.66,1))\n",
    "# quantiles = ((0,0.1),(0.1,0.2),(0.2,0.3),(0.3,0.4),(0.4,0.5),(0.5,0.6),(0.7,0.8),(0.8,0.9),(0.9,1))\n",
    "quantiles = ((0,0.2),(0.2,0.4),(0.4,0.6),(0.6,0.8),(0.8,1))\n",
    "\n",
    "figsize = (20,20)\n",
    "xlims = [-4000,500]\n",
    "\n",
    "for d_idx in trial_dataset_dict.keys():\n",
    "\n",
    "    # Compute normalized position in session\n",
    "    trial_dataset_dict[d_idx].metadata_df = trial_nb_normalization(trial_dataset_dict[d_idx].metadata_df, by_day=False)\n",
    "    trial_dataset_dict[d_idx].metadata_df = trial_nb_quantilization(trial_dataset_dict[d_idx].metadata_df, quantiles = quantiles)\n",
    "\n",
    "    trial_dataset_dict[d_idx].filter_reset()\n",
    "    trial_dataset_dict[d_idx].filterout_subjects([0,1,58,63,313,314,315,318])\n",
    "    trial_dataset_dict[d_idx].filterout_conditions([1])\n",
    "    trial_dataset_dict[d_idx].filter_min_by_session(min_trials = 30)\n",
    "    trial_dataset_dict[d_idx].filter_lastNdays(n = 5)\n",
    "\n",
    "subjects = trial_dataset_dict[0].metadata_df[trial_dataset_dict[0].metadata_df.keep == True].subject_ID.unique()\n",
    "# quantiles = trial_dataset_dict[0].metadata_df[trial_dataset_dict[0].metadata_df.keep == True].trial_nb_quantile.unique()\n",
    "# quantiles_idx = range(len(quantiles))\n",
    "cmap_quant = cm.get_cmap('jet', len(quantiles))\n",
    "\n",
    "fig, axs = plt.subplots(len(subjects)+1, len(trial_dataset_dict), sharex= 'all',\n",
    "    sharey = 'row', squeeze = False , figsize = figsize)\n",
    "\n",
    "for d_idx, trial_dataset in trial_dataset_dict.items():\n",
    "    dataset_means = np.empty([len(subjects),len(quantiles),trial_dataset.data.shape[2]])\n",
    "    dataset_sems = np.empty([len(subjects),len(quantiles),trial_dataset.data.shape[2]])\n",
    "\n",
    "    # Only compute for pre-selected trials via the filtering methods\n",
    "    # CAUTION, perform on deep copies since the following line permanently delete parts of the metadata\n",
    "    trial_dataset.metadata_df = trial_dataset.metadata_df[trial_dataset.metadata_df.keep == True]\n",
    "    time_vec = trial_dataset.get_time_vector()\n",
    "\n",
    "    for s_idx, subject in enumerate(subjects):\n",
    "\n",
    "        for q_idx, quantile in enumerate(quantiles):\n",
    "            # indices of the trials to pick\n",
    "            np_idx = trial_dataset.metadata_df[\n",
    "                ((trial_dataset.metadata_df['trial_nb_quantile'] ==  q_idx) & (trial_dataset.metadata_df['subject_ID'] == subject))\n",
    "                ].index.values\n",
    "            # mean and sems computation\n",
    "            dataset_means[s_idx,q_idx,:] = np.nanmean(trial_dataset.data[np_idx, trial_dataset.colnames_dict[photo_var], :], axis=0)\n",
    "            dataset_sems[s_idx,q_idx,:] = np.nanstd(trial_dataset.data[np_idx, trial_dataset.colnames_dict[photo_var], :], axis=0) / np.sqrt(len(np_idx))\n",
    "\n",
    "            axs[s_idx, d_idx].fill_between(\n",
    "                time_vec,\n",
    "                dataset_means[s_idx,q_idx,:] - dataset_sems[s_idx,q_idx,:],\n",
    "                dataset_means[s_idx,q_idx,:] + dataset_sems[s_idx,q_idx,:],\n",
    "                alpha=0.2,\n",
    "                color = cmap_quant(q_idx)\n",
    "            )\n",
    "            \n",
    "            axs[s_idx, d_idx].plot(\n",
    "                time_vec, \n",
    "                dataset_means[s_idx,q_idx,:],\n",
    "                alpha=0.7,\n",
    "                color = cmap_quant(q_idx),\n",
    "                label = f'{q_idx+1} (n = {len(np_idx)})',\n",
    "                )\n",
    "            axs[s_idx, d_idx].set_xlim(xlims)\n",
    "            axs[s_idx, d_idx].set_title(subject)\n",
    "            axs[s_idx, d_idx].legend()\n",
    "\n",
    "\n",
    "    for q_idx, quantile in enumerate(quantiles):\n",
    "        \n",
    "        group_sem = np.nanstd(dataset_means[:,q_idx,:],0) / np.sqrt(len(subjects))\n",
    "\n",
    "        axs[s_idx+1, d_idx].fill_between(\n",
    "            time_vec,\n",
    "            np.nanmean(dataset_means[:,q_idx,:],0) - dataset_sems[s_idx,q_idx,:],\n",
    "            np.nanmean(dataset_means[:,q_idx,:],0) + dataset_sems[s_idx,q_idx,:],\n",
    "            alpha=0.2,\n",
    "            color = cmap_quant(q_idx)\n",
    "        )\n",
    "        \n",
    "        axs[s_idx+1, d_idx].plot(\n",
    "            time_vec, \n",
    "            np.nanmean(dataset_means[:,q_idx,:],0),\n",
    "            alpha=0.7,\n",
    "            color = cmap_quant(q_idx),\n",
    "            label = f'{q_idx+1} (n = {len(subjects)})',\n",
    "            )\n",
    "        axs[s_idx+1, d_idx].set_xlim(xlims)\n",
    "        axs[s_idx+1, d_idx].set_title('Group Average')\n",
    "        axs[s_idx+1, d_idx].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dataset.metadata_df.groupby(['subject_ID']).agg({'session_nb_norm':list}).session_nb_norm.apply(lambda x: set(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "trial_dataset_dict = dict()\n",
    "for d_idx in photo_dataset_mixed.keys():\n",
    "    trial_dataset_dict[d_idx] = deepcopy(photo_dataset_mixed[d_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from copy import deepcopy\n",
    "\n",
    "photo_var = 'zscored_df_over_f'\n",
    "quantiles = ((0,0.33),(0.33,0.66),(0.66,1))\n",
    "# quantiles = ((0,0.1),(0.1,0.2),(0.2,0.3),(0.3,0.4),(0.4,0.5),(0.5,0.6),(0.7,0.8),(0.8,0.9),(0.9,1))\n",
    "# quantiles = ((0,0.2),(0.2,0.4),(0.4,0.6),(0.6,0.8),(0.8,1))\n",
    "\n",
    "figsize = (20,20)\n",
    "xlims = [-500,500]\n",
    "\n",
    "data_dict_copy = dict()\n",
    "for d_idx in trial_dataset_dict.keys():\n",
    "\n",
    "    trial_dataset_dict[d_idx].filter_reset()\n",
    "    trial_dataset_dict[d_idx].filterout_subjects([0,1,313,314,315,317,318])\n",
    "    trial_dataset_dict[d_idx].filterout_conditions([1])\n",
    "    trial_dataset_dict[d_idx].filter_min_by_session(min_trials = 30)\n",
    "    # trial_dataset_dict[d_idx].filter_lastNdays(n = 5)\n",
    "    data_dict_copy[d_idx] = deepcopy(trial_dataset_dict[d_idx])\n",
    "\n",
    "subjects = trial_dataset_dict[0].metadata_df[trial_dataset_dict[0].metadata_df.keep == True].subject_ID.unique()\n",
    "# quantiles = trial_dataset_dict[0].metadata_df[trial_dataset_dict[0].metadata_df.keep == True].trial_nb_quantile.unique()\n",
    "# quantiles_idx = range(len(quantiles))\n",
    "cmap_quant = cm.get_cmap('jet', len(quantiles))\n",
    "\n",
    "fig, axs = plt.subplots(len(subjects)+1, len(trial_dataset_dict), sharex= 'all',\n",
    "    sharey = 'row', squeeze = False , figsize = figsize)\n",
    "\n",
    "for d_idx, trial_dataset in data_dict_copy.items():\n",
    "    dataset_means = np.empty([len(subjects),len(quantiles),trial_dataset.data.shape[2]])\n",
    "    dataset_sems = np.empty([len(subjects),len(quantiles),trial_dataset.data.shape[2]])\n",
    "\n",
    "    # Only compute for pre-selected trials via the filtering methods\n",
    "    trial_dataset.metadata_df = trial_dataset.metadata_df[trial_dataset.metadata_df.keep == True]\n",
    "    # print(trial_dataset.metadata_df.shape[0],trial_dataset_dict[d_idx].metadata_df.shape[0])\n",
    "    # Compute normalized position in session\n",
    "    trial_dataset.metadata_df = session_nb_normalization(trial_dataset.metadata_df, by_day=False)\n",
    "    trial_dataset.metadata_df = session_nb_quantilization(trial_dataset.metadata_df, quantiles = quantiles)\n",
    "\n",
    "    time_vec = trial_dataset.get_time_vector()\n",
    "\n",
    "    for s_idx, subject in enumerate(subjects):\n",
    "\n",
    "        for q_idx, quantile in enumerate(quantiles):\n",
    "            # indices of the trials to pick\n",
    "            np_idx = trial_dataset.metadata_df[\n",
    "                ((trial_dataset.metadata_df['session_nb_quantile'] ==  q_idx) & (trial_dataset.metadata_df['subject_ID'] == subject))\n",
    "                ].index.values\n",
    "            # mean and sems computation\n",
    "            dataset_means[s_idx,q_idx,:] = np.nanmean(trial_dataset.data[np_idx, trial_dataset.colnames_dict[photo_var], :], axis=0)\n",
    "            dataset_sems[s_idx,q_idx,:] = np.nanstd(trial_dataset.data[np_idx, trial_dataset.colnames_dict[photo_var], :], axis=0) / np.sqrt(len(np_idx))\n",
    "\n",
    "            axs[s_idx, d_idx].fill_between(\n",
    "                time_vec,\n",
    "                dataset_means[s_idx,q_idx,:] - dataset_sems[s_idx,q_idx,:],\n",
    "                dataset_means[s_idx,q_idx,:] + dataset_sems[s_idx,q_idx,:],\n",
    "                alpha=0.2,\n",
    "                color = cmap_quant(q_idx)\n",
    "            )\n",
    "            \n",
    "            axs[s_idx, d_idx].plot(\n",
    "                time_vec, \n",
    "                dataset_means[s_idx,q_idx,:],\n",
    "                alpha=0.7,\n",
    "                color = cmap_quant(q_idx),\n",
    "                label = f'{q_idx+1} (n = {len(np_idx)})',\n",
    "                )\n",
    "            axs[s_idx, d_idx].set_xlim(xlims)\n",
    "            axs[s_idx, d_idx].set_title(subject)\n",
    "            axs[s_idx, d_idx].legend()\n",
    "\n",
    "\n",
    "    for q_idx, quantile in enumerate(quantiles):\n",
    "        \n",
    "        group_sem = np.nanstd(dataset_means[:,q_idx,:],0) / np.sqrt(len(subjects))\n",
    "\n",
    "        axs[s_idx+1, d_idx].fill_between(\n",
    "            time_vec,\n",
    "            np.nanmean(dataset_means[:,q_idx,:],0) - dataset_sems[s_idx,q_idx,:],\n",
    "            np.nanmean(dataset_means[:,q_idx,:],0) + dataset_sems[s_idx,q_idx,:],\n",
    "            alpha=0.2,\n",
    "            color = cmap_quant(q_idx)\n",
    "        )\n",
    "        \n",
    "        axs[s_idx+1, d_idx].plot(\n",
    "            time_vec, \n",
    "            np.nanmean(dataset_means[:,q_idx,:],0),\n",
    "            alpha=0.7,\n",
    "            color = cmap_quant(q_idx),\n",
    "            label = f'{q_idx+1} (n = {len(subjects)})',\n",
    "            )\n",
    "        axs[s_idx+1, d_idx].set_xlim(xlims)\n",
    "        axs[s_idx+1, d_idx].set_title('Group Average')\n",
    "        axs[s_idx+1, d_idx].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import decimate\n",
    "from trialexp.process.pyphotometry.photometry_functional import get_photometry_trials\n",
    "\n",
    "\n",
    "sessions = [session for session in exp_cohort_mixed.sessions if (session.subject_ID == 64 and session.datetime.date() == datetime(2023,2,23).date()) ]\n",
    "\n",
    "df_meta_photo, col_names_numpy, photo_array, photometry_dict = get_photometry_trials(\n",
    "    sessions[1],\n",
    "    conditions_list = None,\n",
    "    cond_aliases = None,\n",
    "    trial_window = [-4000,6000],\n",
    "    trig_on_ev = None,\n",
    "    last_before = None,\n",
    "    baseline_low_pass = 0.001, \n",
    "    low_pass = 45, \n",
    "    median_filt = 5, \n",
    "    motion_corr = True, \n",
    "    df_over_f = True,\n",
    "    z_score = True, # To be implemented\n",
    "    downsampling_factor = 10,\n",
    "    return_full_session = True, \n",
    "    export_vars = ['analog_2','analog_1_df_over_f','analog_1_corrected'],\n",
    "    verbose = False)\n",
    "\n",
    "photometry_dict.keys()\n",
    "\n",
    "\n",
    "vars_to_plot = ['analog_1', 'analog_2', 'analog_1_filt', 'analog_2_filt','analog_1_est_motion', 'analog_1_corrected', 'analog_1_baseline_fluo', 'analog_1_df_over_f', 'zscored_df_over_f']\n",
    "\n",
    "figsize = (20,40)\n",
    "fig, axs = plt.subplots(len(vars_to_plot),1, figsize = figsize)\n",
    "for l, v in enumerate(vars_to_plot):\n",
    "    print(len(photometry_dict[v]) , len(photometry_dict['time']))\n",
    "    if len(photometry_dict[v]) < len(photometry_dict['time']):\n",
    "        axs[l].plot(decimate(photometry_dict['time'],10)/1000, photometry_dict[v])\n",
    "    else:\n",
    "        axs[l].plot(photometry_dict['time']/1000, photometry_dict[v])\n",
    "    axs[l].set_title(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(session.datetime.date(), datetime(2023,2,23).date()) for session in sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigs = [None, 'bar_off', 'bar_off', 'spout', 'US_end_timer']\n",
    "last_befores = [None, None, 'spout', None, None]\n",
    "trial_window = [-4000, 6000]\n",
    "photo_dataset_mixed = dict()\n",
    "photo_dataset_mixed[0] = exp_cohort_mixed.get_photometry_groups(\n",
    "        groups = None, # or use groups variable defined above\n",
    "        conditions_list = conditions_list, \n",
    "        cond_aliases = cond_aliases,\n",
    "        trial_window = trial_window,\n",
    "        when = 'all', \n",
    "        task_names = ['reaching_go_spout_bar_nov22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "        trig_on_ev = trig, # align to the first event of a kind e.g. bar_off\n",
    "        last_before = last_befores[idx],\n",
    "        high_pass = 0.01, \n",
    "        low_pass = 45, \n",
    "        median_filt = 3,\n",
    "        motion_corr = True, \n",
    "        df_over_f = True,\n",
    "        z_score = True, \n",
    "        downsampling_factor = 10, \n",
    "        export_vars = ['analog_1_df_over_f', 'zscored_df_over_f'], \n",
    "        # remove_artifacts = False, # To Deprecate in favor of Exp level artifact clustering\n",
    "        verbose = True) # will plot all the process of remove_artifacts if remove_artifacts == True\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trialexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6482735f0043a190ffe4caf0b320db79ae95bb9719ede4e9819067a592cbc72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
