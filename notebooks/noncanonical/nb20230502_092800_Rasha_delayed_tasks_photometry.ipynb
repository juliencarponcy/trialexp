{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delayed tasks analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert notebook to python\n",
    "```\n",
    "bash\n",
    "jupyter nbconvert \"D:\\OneDrive - Nexus365\\Private_Dropbox\\Projects\\trialexp\\notebooks\\noncanonical\\nb20230202_145400_delayed_tasks_and_316_photometry.ipynb\" --to=\"python\" --output-dir=\"D:\\OneDrive - Nexus365\\Private_Dropbox\\Projects\\trialexp\\notebooks\\noncanonical\" --output=\"nb20230202_145400_delayed_tasks_and_316_photometry\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick analysis of instrumental reaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow for automatic reloading of classes and function when updating the code\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "# Import Session and Experiment class with helper functions\n",
    "from trialexp.process.data_import import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "trial_window = [-2000, 6000] # in ms\n",
    "\n",
    "# time limit around trigger to perform an event\n",
    "# determine successful trials\n",
    "# timelim = [1000, 4000] # in ms\n",
    "\n",
    "# Digital channel nb of the pyphotometry device\n",
    "# on which rsync signal is sent (from pycontrol device)\n",
    "rsync_chan = 2\n",
    "\n",
    "basefolder = Path(os.getcwd()).parent.parent\n",
    "\n",
    "# These must be absolute paths\n",
    "# use this to use within package tasks files (in params)\n",
    "tasksfile = Path(basefolder,'params','tasks_params.csv')\n",
    "# use this to put a local full path\n",
    "#tasksfile = -r'C:/.../tasks_params.csv' \n",
    "\n",
    "# from sample_data\n",
    "\n",
    "# # From jade\n",
    "# photometry_dir = Path('/home/MRC.OX.AC.UK/phar0732/ettin/Data/head-fixed/photometry')\n",
    "# pycontrol_dir = Path('/home/MRC.OX.AC.UK/phar0732/ettin/Data/head-fixed/pycontrol')\n",
    "\n",
    "# From julien-pc\n",
    "photometry_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\pyphotometry\\data\\reaching_go_spout_bar_nov22'\n",
    "pycontrol_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\pycontrol\\reaching_go_spout_bar_nov22'\n",
    "\n",
    "#From laptop\n",
    "# photometry_dir = r'C:\\Users\\phar0732\\Documents\\GitHub\\trialexp\\sample_data\\pyphotometry\\reaching_go_spout_incr_break2_nov22'\n",
    "# pycontrol_dir = r'C:\\Users\\phar0732\\Documents\\GitHub\\trialexp\\sample_data\\pycontrol\\reaching_go_spout_incr_break2_nov22'\n",
    "\n",
    "video_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\videos'\n",
    "tasks = pd.read_csv(tasksfile, usecols=[1, 2, 3, 4], index_col=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all raw text sessions in the indicated folder or a sessions.pkl file\n",
    "# if already existing in folder_path\n",
    "exp_cohort = Experiment(path=pycontrol_dir, int_subject_IDs=True, update=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exp_cohort.sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cohort.sessions[0].datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "exp_cohort.sessions = [session for session in exp_cohort.sessions if (session.subject_ID in [604, 602]) \\\n",
    "                       and (session.datetime.date() in [datetime.date(2023,3,20), datetime.date(2023,3,21), datetime.date(2023,3,22), datetime.date(2023,3,23), datetime.date(2023,3,24), datetime.date(2023,3,27)] )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform extraction of behavioural information by trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cohort.sessions[0].get_session_by_trial(trial_window=trial_window, timelim=[0,1000], tasksfile=tasksfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the whole experimental folder by trials\n",
    "\n",
    "exp_cohort.process_exp_by_trial(trial_window, timelim=None, tasksfile=tasksfile, verbose=True)\n",
    "\n",
    "# Save the file as sessions.pkl in folder_path\n",
    "# exp_cohort.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exp_cohort.sessions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditions for delayed go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cohort.sessions[0].df_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defime each trial type as a dictionary of conditions to be met\n",
    "conditions_dict1 = {'trigger': 'hold_for_water', 'success': True, 'water by spout': True}\n",
    "\n",
    "# conditions_dict2 = {'trigger': 'hold_for_water', 'success': False, 'hold_timer': True}\n",
    "\n",
    "conditions_dict2 = {'trigger': 'hold_for_water', 'success': False, 'break_after_abort': True}\n",
    "\n",
    "# conditions_dict2 = {'trigger': 'hold_for_water', 'spout':False, 'valid': True, 'busy_win_timer': False, 'button_press': False}\n",
    "\n",
    "# Aggregate all condition dictionaries in a list\n",
    "conditions_list = [conditions_dict1, conditions_dict2]\n",
    "# Aliases for conditions\n",
    "cond_aliases = ['Go - hold hit', 'Go - aborted']\n",
    "# Groups as a list of lists\n",
    "# groups = [[280, 281, 282, 289],[295, 282, 284, 285, 292, 297]]\n",
    "groups = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if there is a matching photometry file:\n",
    "exp_cohort.match_sessions_to_files(photometry_dir, ext='ppd')\n",
    "\n",
    "# rsync synchronization pulses matching between behaviour and photometry\n",
    "exp_cohort.sync_photometry_files(2)\n",
    "exp_cohort.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed_mixed_mixed = exp_cohort.behav_events_to_dataset(\n",
    "        groups = None,\n",
    "        conditions_list = conditions_list, \n",
    "        cond_aliases = cond_aliases, \n",
    "        when = 'all', \n",
    "        task_names = ['reaching_go_spout_bar_nov22'], #'reaching_go_spout_incr_break2_nov22' ], #'reaching_go_spout_bar_dual_all_reward_dec22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "        trig_on_ev = None\n",
    "        )\n",
    "\n",
    "ev_dataset_mixed_mixed_mixed.set_trial_window(trial_window=[-4000,6000], unit='milliseconds')\n",
    "# ev_dataset.set_conditions(conditions=condition_list, aliases=cond_aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ev_dataset_mixed_mixed_mixed.metadata_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_by_session = ev_dataset.metadata_df.sort_values(by=['session_nb'], ascending=False).index\n",
    "\n",
    "ev_dataset.metadata_df = ev_dataset.metadata_df.iloc[index_by_session]\n",
    "ev_dataset.data = ev_dataset.data.iloc[index_by_session]\n",
    "\n",
    "ev_dataset.filter_reset()\n",
    "ev_dataset.filter_lastNsessions(3)\n",
    "ev_dataset.filterout_subjects([602,604])\n",
    "\n",
    "ev_dataset.data.loc[:,'first_bar_off_trial_time'] = ev_dataset.data['bar_off_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset.data.loc[:,'first_spout_trial_time'] = ev_dataset.data['spout_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset.plot_raster(keys=['first_bar_off_trial_time','first_spout_trial_time'], module='matplotlib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ev_dataset.plot_raster(keys=['first_bar_off_trial_time', 'first_spout_trial_time'], module='matplotlib')\n",
    "ev_dataset_nocond = deepcopy(ev_dataset)\n",
    "ev_dataset_nocond.filter_min_by_session(10)\n",
    "ev_dataset_nocond.filterout_subjects([602,604])\n",
    "ev_dataset_nocond.filter_lastNsessions(10)\n",
    "ev_dataset_nocond.plot_raster(keys=['first_bar_off_trial_time', 'first_spout_trial_time'], module='matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaviour: Compute distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset.filter_reset()\n",
    "ev_dataset.filter_min_by_session(20)\n",
    "ev_dataset.filterout_subjects([58,61,63,313,314,315])\n",
    "# ev_dataset.filter_lastNsessions(3)\n",
    "\n",
    "\n",
    "dist_as_continuous = ev_dataset.compute_distribution(\n",
    "        trial_window = [-1999, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "# dist_as_continuous.set_conditions(conditions=conditions_list, aliases=cond_aliases)\n",
    "# Remove test files\n",
    "# dist_as_continuous.filterout_subjects([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_as_continuous.heatmap(\n",
    "    vars = ['first_bar_off_dist', 'first_spout_dist'],\n",
    "    time_lim = [-1000, 6000],\n",
    "    colormap = 'jet'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaviour: Plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_as_continuous.filter_reset()\n",
    "ev_dataset.filter_min_by_session(20)\n",
    "ev_dataset.filterout_subjects([58,61,63,313,314,315])\n",
    "ev_dataset.filter_lastNsessions(3)\n",
    "\n",
    "dist_as_continuous = ev_dataset.compute_distribution(\n",
    "        trial_window = [-1999, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "\n",
    "dist_as_continuous.lineplot(\n",
    "    vars = [ 'first_bar_off_dist','first_spout_dist'],\n",
    "    time_lim = [-1000,6000],\n",
    "    error = True,\n",
    "    ylim = None,#[[-0.1,1.6]], #[[-0.1, 0.7]], #[[-0.1, 1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "    colormap = 'jet',\n",
    "    legend = True,\n",
    "    plot_subjects = True,\n",
    "    plot_groups = True,\n",
    "    figsize = (15,10),\n",
    "    dpi = 100,\n",
    "    verbose = False)\n",
    "# Return a count of overall number of trials\n",
    "dist_as_continuous.metadata_df['keep'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match and synchronize photometry to behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if there is a matching photometry file:\n",
    "exp_cohort.match_sessions_to_files(photometry_dir, ext='ppd')\n",
    "\n",
    "# rsync synchronization pulses matching between behaviour and photometry\n",
    "exp_cohort.sync_photometry_files(2)\n",
    "exp_cohort.save()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop to plot photometry trials triggered on different events\n",
    "- 1: Trial onset (CS-onset = hold period start)\n",
    "- 2: First bar_off\n",
    "- 3: Last bar_off before spout\n",
    "- 4: First spout\n",
    "- 5: Reward (US_end_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigs = [None, 'bar_off', 'bar_off', 'spout', 'US_end_timer']\n",
    "last_befores = [None, None, 'spout', None, None]\n",
    "\n",
    "photo_dataset = dict()\n",
    "for idx, trig in enumerate(trigs):\n",
    "    photo_dataset[idx] = exp_cohort.get_photometry_groups(\n",
    "            groups = None, # or use groups variable defined above\n",
    "            conditions_list = conditions_list, \n",
    "            cond_aliases = cond_aliases,\n",
    "            trial_window = trial_window,\n",
    "            when = 'all', \n",
    "            task_names = ['reaching_go_spout_bar_nov22'],\n",
    "            trig_on_ev = trig, # align to the first event of a kind e.g. bar_off\n",
    "            last_before = last_befores[idx],\n",
    "            high_pass = None, \n",
    "            low_pass = 45, \n",
    "            median_filt = 3,\n",
    "            motion_corr = True, \n",
    "            df_over_f = True,\n",
    "            z_score = True, \n",
    "            downsampling_factor = 10, \n",
    "            export_vars = ['analog_1_df_over_f', 'zscored_df_over_f'], \n",
    "            # remove_artifacts = False, # To Deprecate in favor of Exp level artifact clustering\n",
    "            verbose = True) # will plot all the process of remove_artifacts if remove_artifacts == True\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot photometry trials triggered on different events based on above extraction\n",
    "- 1: Trial onset (CS-onset = hold period start)\n",
    "- 2: First bar_off\n",
    "- 3: Last bar_off before spout\n",
    "- 4: First spout\n",
    "- 5: Reward (US_end_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, trig in enumerate(trigs):\n",
    "\n",
    "    # keep only 316\n",
    "    photo_dataset[idx].filter_reset()\n",
    "    photo_dataset[idx].filterout_subjects([0,1,58,61,63,313,314,315,318])\n",
    "    photo_dataset[idx].filter_min_by_session(min_trials = 10)\n",
    "    photo_dataset[idx].filter_lastNsessions(n = 3)\n",
    "    if idx == 4:\n",
    "        photo_dataset[idx].filterout_conditions(1)\n",
    "    photo_dataset[idx].lineplot(\n",
    "        vars = ['zscored_df_over_f'],\n",
    "        time_lim = [-3000, 6000],\n",
    "        # time_unit = 'seconds',\n",
    "        ylim = [[-1, 5]],# [[-0.004, 0.006]],#[[-0.03, 0.1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "        error = True,\n",
    "        colormap = 'jet',\n",
    "        legend = True,\n",
    "        plot_subjects = True,\n",
    "        plot_groups = True,\n",
    "        liney0 = False,\n",
    "        linex0 = True,\n",
    "        figsize = (15, 5),\n",
    "        dpi = 100,\n",
    "        verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same plot, heatmap version:\n",
    "\n",
    "- 1: Trial onset (CS-onset = hold period start)\n",
    "- 2: First bar_off\n",
    "- 3: Last bar_off before spout\n",
    "- 4: First spout\n",
    "- 5: Reward (US_end_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_labels = ('Cue_onset','First_mov','Mov_bef_spout','Spout','Reward')\n",
    "\n",
    "for idx, trig in enumerate(trigs):\n",
    "    photo_dataset[idx].filter_reset()\n",
    "    photo_dataset[idx].filterout_subjects([0,1,61,63,313,314,315,317,318])\n",
    "    photo_dataset[idx].filterout_dates([datetime(2023,2,24).date(), datetime(2023,2,23).date()])\n",
    "    photo_dataset[idx].filter_lastNsessions(n = 1)\n",
    "    fig = photo_dataset[idx].heatmap(\n",
    "        vars = ['zscored_df_over_f'],\n",
    "        time_lim = [-3000, 6000],\n",
    "        clim_pctile = None,\n",
    "        colormap = 'jet'\n",
    "    )   \n",
    "    photo_dataset[idx].filter_reset()\n",
    "\n",
    "\n",
    "    file_path = 'C:\\\\Users\\\\phar0732\\\\Documents\\\\GitHub\\\\trialexp\\\\outputs\\\\' + 'photo_heatmap_' + phase_labels[idx] + '.pdf'\n",
    "    fig.savefig(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_dataset[idx].metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From julien-pc\n",
    "photometry_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\_Other\\test_folder\\delayed_go\\pyphotometry\\delayed_go_dual_2022'\n",
    "pycontrol_dir = r'\\\\ettin\\Magill_Lab\\Julien\\Data\\head-fixed\\_Other\\test_folder\\delayed_go\\pycontrol\\delayed_go_dual_2022'\n",
    "trial_window=[-3000,6000]\n",
    "exp_cohort_mixed = Experiment(path=pycontrol_dir, int_subject_IDs=True, update=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cohort_mixed.process_exp_by_trial(trial_window, timelim=None, tasksfile=tasksfile, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defime each trial type as a dictionary of conditions to be met\n",
    "conditions_dict1 = {'trigger': 'hold_for_water', 'success': True, 'water by spout': True}\n",
    "\n",
    "# conditions_dict2 = {'trigger': 'hold_for_water', 'success': False, 'hold_timer': True}\n",
    "\n",
    "conditions_dict2 = {'trigger': 'hold_for_water', 'success': False, 'break_after_abort': True}\n",
    "\n",
    "# conditions_dict2 = {'trigger': 'hold_for_water', 'spout':False, 'valid': True, 'busy_win_timer': False, 'button_press': False}\n",
    "\n",
    "# Aggregate all condition dictionaries in a list\n",
    "conditions_list = [conditions_dict1, conditions_dict2]\n",
    "# Aliases for conditions\n",
    "cond_aliases = ['Go - hold hit', 'Go - aborted']\n",
    "# Groups as a list of lists\n",
    "# groups = [[280, 281, 282, 289],[295, 282, 284, 285, 292, 297]]\n",
    "groups = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed = exp_cohort_mixed.behav_events_to_dataset(\n",
    "        groups = None,\n",
    "        conditions_list = conditions_list, \n",
    "        cond_aliases = cond_aliases, \n",
    "        when = 'all', \n",
    "        task_names = ['reaching_go_spout_bar_nov22' ,'reaching_go_spout_bar_dual_dec22','reaching_go_spout_bar_all_reward_dec22','reaching_go_spout_bar_all_reward_dec22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "        trig_on_ev = None)\n",
    "\n",
    "ev_dataset_mixed.set_trial_window(trial_window=[-3000,6000], unit='milliseconds')\n",
    "# ev_dataset.set_conditions(conditions=condition_list, aliases=cond_aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_by_session = ev_dataset_mixed.metadata_df.sort_values(by=['session_nb'], ascending=False).index\n",
    "\n",
    "ev_dataset_mixed.metadata_df = ev_dataset_mixed.metadata_df.iloc[index_by_session]\n",
    "ev_dataset_mixed.data = ev_dataset_mixed.data.iloc[index_by_session]\n",
    "\n",
    "ev_dataset_mixed.filter_reset()\n",
    "ev_dataset_mixed.filter_lastNsessions(5)\n",
    "ev_dataset_mixed.filterout_subjects([58,61,63,313,314,315])\n",
    "\n",
    "ev_dataset_mixed.data.loc[:,'first_bar_off_trial_time'] = ev_dataset_mixed.data['bar_off_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed.data.loc[:,'first_spout_trial_time'] = ev_dataset_mixed.data['spout_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed.plot_raster(keys=['first_bar_off_trial_time','first_spout_trial_time'], module='matplotlib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed.filter_reset()\n",
    "ev_dataset_mixed.filter_min_by_session(20)\n",
    "ev_dataset_mixed.filterout_subjects([313,314,315])\n",
    "# ev_dataset.filter_lastNsessions(3)\n",
    "\n",
    "\n",
    "dist_as_continuous_mixed = ev_dataset_mixed.compute_distribution(\n",
    "        trial_window = [-3000, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "# dist_as_continuous.set_conditions(conditions=conditions_list, aliases=cond_aliases)\n",
    "# Remove test files\n",
    "# dist_as_continuous.filterout_subjects([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed_no_cond = exp_cohort_mixed.behav_events_to_dataset(\n",
    "        groups = None,\n",
    "        conditions_list = None, \n",
    "        cond_aliases = None, \n",
    "        when = 'all', \n",
    "        task_names = ['reaching_go_spout_bar_nov22' ,'reaching_go_spout_bar_dual_dec22','reaching_go_spout_bar_all_reward_dec22','reaching_go_spout_bar_all_reward_dec22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "        trig_on_ev = None)\n",
    "\n",
    "ev_dataset_mixed_no_cond.set_trial_window(trial_window=[-2000,6000], unit='milliseconds')\n",
    "# ev_dataset.set_con\n",
    "\n",
    "index_by_session = ev_dataset_mixed_no_cond.metadata_df.sort_values(by=['session_nb'], ascending=True).index\n",
    "\n",
    "ev_dataset_mixed_no_cond.metadata_df = ev_dataset_mixed_no_cond.metadata_df.iloc[index_by_session]\n",
    "ev_dataset_mixed_no_cond.data = ev_dataset_mixed_no_cond.data.iloc[index_by_session]\n",
    "\n",
    "ev_dataset_mixed_no_cond.filter_reset()\n",
    "# ev_dataset_mixed_no_cond.filter_lastNsessions(10)\n",
    "ev_dataset_mixed_no_cond.filterout_subjects([58,61,63,313,314,315])\n",
    "\n",
    "ev_dataset_mixed_no_cond.data.loc[:,'first_bar_off_trial_time'] = ev_dataset_mixed_no_cond.data['bar_off_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed_no_cond.data.loc[:,'first_spout_trial_time'] = ev_dataset_mixed_no_cond.data['spout_trial_time'].apply(lambda x: [find_min_time_list(x)])\n",
    "ev_dataset_mixed_no_cond.plot_raster(keys=['first_bar_off_trial_time','first_spout_trial_time'], module='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed_no_cond.filter_reset()\n",
    "# ev_dataset_mixed_no_cond.filter_lastNsessions(10)\n",
    "ev_dataset_mixed_no_cond.filter_min_by_session(40)\n",
    "ev_dataset_mixed_no_cond.filterout_subjects([313,314,315])\n",
    "\n",
    "dist_as_continuous_mixed_no_cond = ev_dataset_mixed_no_cond.compute_distribution(\n",
    "        trial_window = [-2000, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "\n",
    "dist_as_continuous_mixed_no_cond.heatmap(\n",
    "    vars = ['bar_off_dist', 'spout_dist'],\n",
    "    time_lim = [-2000, 6000],\n",
    "    colormap = 'jet',\n",
    "    figsize = (7, 5),\n",
    "    dpi = 80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_dataset_mixed_no_cond.filter_reset()\n",
    "ev_dataset_mixed_no_cond.filter_min_by_session(40)\n",
    "ev_dataset_mixed_no_cond.filterout_subjects([58,61,63,313,314,315])\n",
    "\n",
    "# ev_dataset_mixed_no_cond.filter_lastNdays(3)\n",
    "ev_dataset_mixed_no_cond.filter_firstNsessions(3)\n",
    "\n",
    "dist_as_continuous_mixed_no_cond = ev_dataset_mixed_no_cond.compute_distribution(\n",
    "        trial_window = [-2000, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "\n",
    "fig, axs, out_df = dist_as_continuous_mixed_no_cond.lineplot(\n",
    "    vars = [ 'first_bar_off_dist','first_spout_dist'],\n",
    "    time_lim = [-2000,6000],\n",
    "    error = True,\n",
    "    ylim =[[-0.0,1.5],[-0.0,1]], #[[-0.1, 0.7]], #[[-0.1, 1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "    colormap = 'jet',\n",
    "    legend = False,\n",
    "    plot_subjects = True,\n",
    "    plot_groups = True,\n",
    "    figsize = (7,5),\n",
    "    dpi = 80,\n",
    "    verbose = False)\n",
    "\n",
    "fig.savefig(r'C:\\Users\\phar0732\\Documents\\GitHub\\trialexp\\outputs\\first_ev_dist_first3s.pdf')\n",
    "\n",
    "fig = ev_dataset_mixed_no_cond.plot_raster(\n",
    "    keys=['first_bar_off_trial_time','first_spout_trial_time'], \n",
    "    module='matplotlib',\n",
    "    figsize=(3.25,5))\n",
    "\n",
    "fig.savefig(r'C:\\Users\\phar0732\\Documents\\GitHub\\trialexp\\outputs\\first_ev_raster_first3s.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_as_continuous_mixed.heatmap(\n",
    "    vars = ['first_bar_off_dist', 'first_spout_dist'],\n",
    "    time_lim = [-1000, 6000],\n",
    "    colormap = 'jet'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaviour: Plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_as_continuous_mixed.filter_reset()\n",
    "ev_dataset_mixed.filter_min_by_session(10)\n",
    "ev_dataset_mixed.filterout_subjects([58,61,63,313,314,315])\n",
    "ev_dataset_mixed.filter_lastNsessions(3)\n",
    "\n",
    "dist_as_continuous_mixed = ev_dataset_mixed.compute_distribution(\n",
    "        trial_window = [-3000, 6000],\n",
    "        bin_size = 100, \n",
    "        normalize = True,\n",
    "        per_session = True,\n",
    "        out_as_continuous = True)\n",
    "\n",
    "dist_as_continuous_mixed.lineplot(\n",
    "    vars = [ 'first_bar_off_dist','first_spout_dist'],\n",
    "    time_lim = [-1000,6000],\n",
    "    error = True,\n",
    "    ylim = None,#[[-0.1,1.6]], #[[-0.1, 0.7]], #[[-0.1, 1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "    colormap = 'jet',\n",
    "    legend = True,\n",
    "    plot_subjects = True,\n",
    "    plot_groups = True,\n",
    "    figsize = (15,10),\n",
    "    dpi = 100,\n",
    "    verbose = False)\n",
    "# Return a count of overall number of trials\n",
    "dist_as_continuous_mixed.metadata_df['keep'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if there is a matching photometry file:\n",
    "exp_cohort_mixed.match_sessions_to_files(photometry_dir, ext='ppd')\n",
    "\n",
    "# rsync synchronization pulses matching between behaviour and photometry\n",
    "exp_cohort_mixed.sync_photometry_files(2)\n",
    "exp_cohort_mixed.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigs = [None, 'bar_off', 'bar_off', 'spout', 'US_end_timer']\n",
    "last_befores = [None, None, 'spout', None, None]\n",
    "trial_window = [-4000, 6000]\n",
    "photo_dataset_mixed = dict()\n",
    "for idx, trig in enumerate(trigs):\n",
    "    photo_dataset_mixed[idx] = exp_cohort_mixed.get_photometry_groups(\n",
    "            groups = None, # or use groups variable defined above\n",
    "            conditions_list = conditions_list, \n",
    "            cond_aliases = cond_aliases,\n",
    "            trial_window = trial_window,\n",
    "            when = 'all', \n",
    "            task_names = ['reaching_go_spout_bar_nov22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "            trig_on_ev = trig, # align to the first event of a kind e.g. bar_off\n",
    "            last_before = last_befores[idx],\n",
    "            high_pass = 0.01, \n",
    "            low_pass = 45, \n",
    "            median_filt = 3,\n",
    "            motion_corr = True, \n",
    "            df_over_f = True,\n",
    "            z_score = True, \n",
    "            downsampling_factor = 10, \n",
    "            export_vars = ['analog_1_df_over_f', 'zscored_df_over_f'], \n",
    "            # remove_artifacts = False, # To Deprecate in favor of Exp level artifact clustering\n",
    "            verbose = True) # will plot all the process of remove_artifacts if remove_artifacts == True\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot photometry trials triggered on different events based on above extraction\n",
    "- 1: Trial onset (CS-onset = hold period start)\n",
    "- 2: First bar_off\n",
    "- 3: Last bar_off before spout\n",
    "- 4: First spout\n",
    "- 5: Reward (US_end_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_labels = ('Cue_onset','First_mov','Mov_bef_spout','Spout','Reward')\n",
    "\n",
    "for idx, trig in enumerate(trigs):\n",
    "\n",
    "    photo_dataset_mixed[idx].filter_reset()\n",
    "    photo_dataset_mixed[idx].filterout_subjects([0,1,58,63,313,314,315,318])\n",
    "    photo_dataset_mixed[idx].filter_min_by_session(min_trials = 10)\n",
    "    photo_dataset_mixed[idx].filter_lastNdays(n = 3)\n",
    "    if idx == 4:\n",
    "        figsize = (9.75, 5)\n",
    "    else:\n",
    "        figsize = (15, 5)\n",
    "\n",
    "    fig, axs, out_df = photo_dataset_mixed[idx].lineplot(\n",
    "        vars = ['zscored_df_over_f'],\n",
    "        time_lim = [-500, 500],\n",
    "        # time_unit = 'seconds',\n",
    "        ylim = [[-1, 5]],# [[-0.004, 0.006]],#[[-0.03, 0.1]],#,[-0.005, 0.007]],#[[-0.001, 0.0011],[-0.001, 0.0011]],\n",
    "        error = True,\n",
    "        colormap = 'jet',\n",
    "        legend = True,\n",
    "        plot_subjects = True,\n",
    "        plot_groups = True,\n",
    "        liney0 = False,\n",
    "        linex0 = True,\n",
    "        figsize = figsize,\n",
    "        dpi = 100,\n",
    "        verbose = False)\n",
    "\n",
    "    file_path = 'C:\\\\Users\\\\phar0732\\\\Documents\\\\GitHub\\\\trialexp\\\\outputs\\\\' + 'photo_ave_' + phase_labels[idx] + '.pdf'\n",
    "    fig.savefig(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, trig in enumerate(trigs):\n",
    "#     photo_dataset_mixed[idx].filter_reset()\n",
    "#     photo_dataset_mixed[idx].filterout_subjects([0,1,313,314,315,318])\n",
    "#     photo_dataset_mixed[idx].filter_min_by_session(min_trials = 30)\n",
    "#     photo_dataset_mixed[idx].filter_lastNsessions(n = 3)\n",
    "#     photo_dataset_mixed[idx].heatmap(\n",
    "#         vars = ['zscored_df_over_f'],\n",
    "#         time_lim = [-1000, 1000],\n",
    "#         clim_pctile = None,\n",
    "#         colormap = 'jet'\n",
    "#     )   \n",
    "#     photo_dataset_mixed[idx].filter_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.loc[:,'date'] = metadata_df['datetime'].apply(lambda d: d.date())\n",
    "dates = metadata_df.loc[:,'date'].unique()\n",
    "dates_norm = np.linspace(0,1,len(dates))\n",
    "dict(zip(dates,dates_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trial_nb_by_day(row, max_trial_nb_by_day):\n",
    "    '''\n",
    "    Helper function to use with apply() on a metadata_df DataFrame.\n",
    "    It re-computes the trial_nb based on total trial in a day in case\n",
    "    of multiple sessions in one day.\n",
    "    '''\n",
    "    # list sessions numbers for the day for this subject\n",
    "\n",
    "    # inefficient to put here, slow down computation\n",
    "    sessions_nb = max_trial_nb_by_day.loc[\n",
    "        (row.subject_ID, row.date, max_trial_nb_by_day.index.get_level_values(2)),:].index.get_level_values(2).values\n",
    "    \n",
    "    sessions_nb = list(sessions_nb)\n",
    "\n",
    "    if row.session_nb == sessions_nb[0]:\n",
    "        # print('row == session_nb')\n",
    "        return row.trial_nb\n",
    "    else:\n",
    "\n",
    "        # return the number of trials before this session on the day\n",
    "        prev_trial_nb = max_trial_nb_by_day.loc[(row.subject_ID, row.date, sessions_nb[:sessions_nb.index(row.session_nb)]),'trial_nb'].cumsum().values[-1]\n",
    "        # print(prev_trial_nb)\n",
    "        return row.trial_nb + prev_trial_nb\n",
    "\n",
    "\n",
    "def trial_nb_normalization(metadata_df, by_day: bool = False):\n",
    "    '''\n",
    "    Compute trial_nb normalized position by session, or by day (if by_day = True) \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        metadata_df: pd.DataFrame\n",
    "            The DataFrame containing metadata for each trial of the trial_dataset\n",
    "        by_day: bool\n",
    "            If True, aggregates trials from sessions performed on the same day to\n",
    "            compute their normalized position\n",
    "    Return:\n",
    "    -------\n",
    "        metadata_df: pd.DataFrame\n",
    "\n",
    "    '''\n",
    "\n",
    "    if by_day:\n",
    "        metadata_df.loc[:,'date'] = metadata_df['datetime'].apply(lambda d: d.date())\n",
    "        max_trial_nb_by_day = metadata_df.groupby(['subject_ID', 'date', 'session_nb']).agg({'trial_nb':'max'})\n",
    "\n",
    "        metadata_df.loc[:,'trial_nb_day'] = metadata_df.apply(lambda x: compute_trial_nb_by_day(x, max_trial_nb_by_day), axis=1)\n",
    "       \n",
    "        max_trial_nb_by_day = max_trial_nb_by_day.groupby(['subject_ID', 'date']).agg({'trial_nb':'sum'})\n",
    "\n",
    "        metadata_df.loc[:,'trial_nb_norm'] = metadata_df.apply(lambda x: x.trial_nb_day / max_trial_nb_by_day.loc[(x['subject_ID'], x['date'])], axis=1)\n",
    "\n",
    "    else:\n",
    "        max_trial_nb = metadata_df.groupby(['subject_ID','session_nb']).agg({'trial_nb':'max'})\n",
    "        metadata_df.loc[:,'trial_nb_norm'] = metadata_df.apply(lambda x: x.trial_nb / max_trial_nb.loc[(x['subject_ID'], x['session_nb'])], axis=1)\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "def session_nb_normalization(metadata_df, by_day: bool = False):\n",
    "    '''\n",
    "    Compute session_nb normalized position by session, or by day (if by_day = True) \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        metadata_df: pd.DataFrame\n",
    "            The DataFrame containing metadata for each trial of the trial_dataset\n",
    "        by_day: bool\n",
    "            If True, aggregates sessions performed on the same day to\n",
    "            compute their normalized position\n",
    "    Return:\n",
    "    -------\n",
    "        metadata_df: pd.DataFrame\n",
    "\n",
    "    '''\n",
    "    metadata_df.loc[:,'date'] = metadata_df['datetime'].apply(lambda d: d.date())\n",
    "\n",
    "    if by_day:\n",
    "        dates_norm_dict = dict()\n",
    "        for subject_ID in metadata_df.subject_ID.unique():\n",
    "\n",
    "            dates = metadata_df.loc[metadata_df.subject_ID == subject_ID,'date'].unique()\n",
    "            dates_norm = np.linspace(0,1,len(dates))\n",
    "            dates_norm_dict[subject_ID] = dict(zip(dates,dates_norm))\n",
    "\n",
    "        metadata_df.loc[:,'session_nb_norm'] = metadata_df.apply(lambda x: dates_norm_dict[x.subject_ID][x.date], axis=1)\n",
    "\n",
    "    else:\n",
    "        dates_norm_dict = dict()\n",
    "        for subject_ID in metadata_df.subject_ID.unique():\n",
    "\n",
    "            sessions = metadata_df.loc[metadata_df.subject_ID == subject_ID, 'session_nb'].unique()\n",
    "            sessions_norm = np.linspace(0,1,len(sessions))\n",
    "            dates_norm_dict[subject_ID] = dict(zip(sessions,sessions_norm))\n",
    "\n",
    "        metadata_df.loc[:,'session_nb_norm'] = metadata_df.apply(lambda x: dates_norm_dict[x.subject_ID][x.session_nb], axis=1)\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "# def trial_nb_normalization_by_day(metadata_df):\n",
    "\n",
    "#     max_trial_nb = metadata_df.groupby(['subject_ID','session_nb']).agg({'trial_nb':'max'})\n",
    "#     metadata_df['trial_nb_norm'] = metadata_df.apply(lambda x: x.trial_nb / max_trial_nb.loc[(x['subject_ID'], x['session_nb'])], axis=1)\n",
    "#     return metadata_df\n",
    "\n",
    "def trial_nb_quantilization(metadata_df: pd.DataFrame, quantiles: tuple):\n",
    "    '''\n",
    "    Assign quantile index to each trial based on the trial_nb_norm value\n",
    "    computed by the trial_nb_normalization(metadata_df) method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        metadata_df: pd.DataFrame\n",
    "            The DataFrame containing metadata for each trial of the dataset\n",
    "        quantiles: tuple\n",
    "            A tuple of 2 entries tuples indicating the limits of the desired quantiles.\n",
    "            The lower limit is not included while the upper limit is.\n",
    "  \n",
    "        example:\n",
    "\n",
    "        >> metadata_df = trial_nb_normalization(metadata_df)\n",
    "        # Break down the trial numbers into 3 thirds\n",
    "        >> quantiles = ((0,0.33),(0.33,0.66),(0.66,1))\n",
    "        >> metadata_df = trial_nb_quantilization(metadata_df, quantiles = quantiles)\n",
    "\n",
    "    \n",
    "    Return the same DataFrame with the correspondign quantile indices\n",
    "    '''\n",
    "\n",
    "    if 'trial_nb_norm' not in metadata_df.columns:\n",
    "        raise Exception('You need to compute first trial_nb_normalization(metadata_df)')\n",
    "\n",
    "    metadata_df['trial_nb_quantile'] = nan\n",
    "    for q_nb, quantile_lim in enumerate(quantiles):\n",
    "        metadata_df.loc[((metadata_df['trial_nb_norm'] > quantile_lim[0]) & (metadata_df['trial_nb_norm'] <= quantile_lim[1])) , 'trial_nb_quantile'] = q_nb\n",
    "\n",
    "    metadata_df['trial_nb_quantile'] = metadata_df['trial_nb_quantile'].astype(int)\n",
    "\n",
    "    return metadata_df\n",
    "\n",
    "def session_nb_quantilization(metadata_df: pd.DataFrame, quantiles: tuple):\n",
    "    '''\n",
    "    Assign quantile index to each trial based on the trial_nb_norm value\n",
    "    computed by the trial_nb_normalization(metadata_df) method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        metadata_df: pd.DataFrame\n",
    "            The DataFrame containing metadata for each trial of the dataset\n",
    "        quantiles: tuple\n",
    "            A tuple of 2 entries tuples indicating the limits of the desired quantiles.\n",
    "            The lower limit is not included while the upper limit is.\n",
    "  \n",
    "        example:\n",
    "\n",
    "        >> metadata_df = trial_nb_normalization(metadata_df)\n",
    "        # Break down the trial numbers into 3 thirds\n",
    "        >> quantiles = ((0,0.33),(0.33,0.66),(0.66,1))\n",
    "        >> metadata_df = trial_nb_quantilization(metadata_df, quantiles = quantiles)\n",
    "\n",
    "    \n",
    "    Return the same DataFrame with the correspondign quantile indices\n",
    "    '''\n",
    "\n",
    "    if 'session_nb_norm' not in metadata_df.columns:\n",
    "        raise Exception('You need to compute first session_nb_normalization(metadata_df)')\n",
    "\n",
    "    metadata_df['session_nb_quantile'] = nan\n",
    "    for q_nb, quantile_lim in enumerate(quantiles):\n",
    "        metadata_df.loc[((metadata_df['session_nb_norm'] >= quantile_lim[0]) & (metadata_df['session_nb_norm'] <= quantile_lim[1])) , 'session_nb_quantile'] = q_nb\n",
    "\n",
    "    metadata_df['session_nb_quantile'] = metadata_df['session_nb_quantile'].astype(int)\n",
    "\n",
    "    return metadata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "trial_dataset_dict = dict()\n",
    "for d_idx in photo_dataset_mixed.keys():\n",
    "    trial_dataset_dict[d_idx] = deepcopy(photo_dataset_mixed[d_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "photo_var = 'zscored_df_over_f'\n",
    "# quantiles = ((0,0.33),(0.33,0.66),(0.66,1))\n",
    "# quantiles = ((0,0.1),(0.1,0.2),(0.2,0.3),(0.3,0.4),(0.4,0.5),(0.5,0.6),(0.7,0.8),(0.8,0.9),(0.9,1))\n",
    "quantiles = ((0,0.2),(0.2,0.4),(0.4,0.6),(0.6,0.8),(0.8,1))\n",
    "\n",
    "figsize = (20,20)\n",
    "xlims = [-4000,500]\n",
    "\n",
    "for d_idx in trial_dataset_dict.keys():\n",
    "\n",
    "    # Compute normalized position in session\n",
    "    trial_dataset_dict[d_idx].metadata_df = trial_nb_normalization(trial_dataset_dict[d_idx].metadata_df, by_day=False)\n",
    "    trial_dataset_dict[d_idx].metadata_df = trial_nb_quantilization(trial_dataset_dict[d_idx].metadata_df, quantiles = quantiles)\n",
    "\n",
    "    trial_dataset_dict[d_idx].filter_reset()\n",
    "    trial_dataset_dict[d_idx].filterout_subjects([0,1,58,63,313,314,315,318])\n",
    "    trial_dataset_dict[d_idx].filterout_conditions([1])\n",
    "    trial_dataset_dict[d_idx].filter_min_by_session(min_trials = 30)\n",
    "    trial_dataset_dict[d_idx].filter_lastNdays(n = 5)\n",
    "\n",
    "subjects = trial_dataset_dict[0].metadata_df[trial_dataset_dict[0].metadata_df.keep == True].subject_ID.unique()\n",
    "# quantiles = trial_dataset_dict[0].metadata_df[trial_dataset_dict[0].metadata_df.keep == True].trial_nb_quantile.unique()\n",
    "# quantiles_idx = range(len(quantiles))\n",
    "cmap_quant = cm.get_cmap('jet', len(quantiles))\n",
    "\n",
    "fig, axs = plt.subplots(len(subjects)+1, len(trial_dataset_dict), sharex= 'all',\n",
    "    sharey = 'row', squeeze = False , figsize = figsize)\n",
    "\n",
    "for d_idx, trial_dataset in trial_dataset_dict.items():\n",
    "    dataset_means = np.empty([len(subjects),len(quantiles),trial_dataset.data.shape[2]])\n",
    "    dataset_sems = np.empty([len(subjects),len(quantiles),trial_dataset.data.shape[2]])\n",
    "\n",
    "    # Only compute for pre-selected trials via the filtering methods\n",
    "    # CAUTION, perform on deep copies since the following line permanently delete parts of the metadata\n",
    "    trial_dataset.metadata_df = trial_dataset.metadata_df[trial_dataset.metadata_df.keep == True]\n",
    "    time_vec = trial_dataset.get_time_vector()\n",
    "\n",
    "    for s_idx, subject in enumerate(subjects):\n",
    "\n",
    "        for q_idx, quantile in enumerate(quantiles):\n",
    "            # indices of the trials to pick\n",
    "            np_idx = trial_dataset.metadata_df[\n",
    "                ((trial_dataset.metadata_df['trial_nb_quantile'] ==  q_idx) & (trial_dataset.metadata_df['subject_ID'] == subject))\n",
    "                ].index.values\n",
    "            # mean and sems computation\n",
    "            dataset_means[s_idx,q_idx,:] = np.nanmean(trial_dataset.data[np_idx, trial_dataset.colnames_dict[photo_var], :], axis=0)\n",
    "            dataset_sems[s_idx,q_idx,:] = np.nanstd(trial_dataset.data[np_idx, trial_dataset.colnames_dict[photo_var], :], axis=0) / np.sqrt(len(np_idx))\n",
    "\n",
    "            axs[s_idx, d_idx].fill_between(\n",
    "                time_vec,\n",
    "                dataset_means[s_idx,q_idx,:] - dataset_sems[s_idx,q_idx,:],\n",
    "                dataset_means[s_idx,q_idx,:] + dataset_sems[s_idx,q_idx,:],\n",
    "                alpha=0.2,\n",
    "                color = cmap_quant(q_idx)\n",
    "            )\n",
    "            \n",
    "            axs[s_idx, d_idx].plot(\n",
    "                time_vec, \n",
    "                dataset_means[s_idx,q_idx,:],\n",
    "                alpha=0.7,\n",
    "                color = cmap_quant(q_idx),\n",
    "                label = f'{q_idx+1} (n = {len(np_idx)})',\n",
    "                )\n",
    "            axs[s_idx, d_idx].set_xlim(xlims)\n",
    "            axs[s_idx, d_idx].set_title(subject)\n",
    "            axs[s_idx, d_idx].legend()\n",
    "\n",
    "\n",
    "    for q_idx, quantile in enumerate(quantiles):\n",
    "        \n",
    "        group_sem = np.nanstd(dataset_means[:,q_idx,:],0) / np.sqrt(len(subjects))\n",
    "\n",
    "        axs[s_idx+1, d_idx].fill_between(\n",
    "            time_vec,\n",
    "            np.nanmean(dataset_means[:,q_idx,:],0) - dataset_sems[s_idx,q_idx,:],\n",
    "            np.nanmean(dataset_means[:,q_idx,:],0) + dataset_sems[s_idx,q_idx,:],\n",
    "            alpha=0.2,\n",
    "            color = cmap_quant(q_idx)\n",
    "        )\n",
    "        \n",
    "        axs[s_idx+1, d_idx].plot(\n",
    "            time_vec, \n",
    "            np.nanmean(dataset_means[:,q_idx,:],0),\n",
    "            alpha=0.7,\n",
    "            color = cmap_quant(q_idx),\n",
    "            label = f'{q_idx+1} (n = {len(subjects)})',\n",
    "            )\n",
    "        axs[s_idx+1, d_idx].set_xlim(xlims)\n",
    "        axs[s_idx+1, d_idx].set_title('Group Average')\n",
    "        axs[s_idx+1, d_idx].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dataset.metadata_df.groupby(['subject_ID']).agg({'session_nb_norm':list}).session_nb_norm.apply(lambda x: set(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "trial_dataset_dict = dict()\n",
    "for d_idx in photo_dataset_mixed.keys():\n",
    "    trial_dataset_dict[d_idx] = deepcopy(photo_dataset_mixed[d_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from copy import deepcopy\n",
    "\n",
    "photo_var = 'zscored_df_over_f'\n",
    "quantiles = ((0,0.33),(0.33,0.66),(0.66,1))\n",
    "# quantiles = ((0,0.1),(0.1,0.2),(0.2,0.3),(0.3,0.4),(0.4,0.5),(0.5,0.6),(0.7,0.8),(0.8,0.9),(0.9,1))\n",
    "# quantiles = ((0,0.2),(0.2,0.4),(0.4,0.6),(0.6,0.8),(0.8,1))\n",
    "\n",
    "figsize = (20,20)\n",
    "xlims = [-500,500]\n",
    "\n",
    "data_dict_copy = dict()\n",
    "for d_idx in trial_dataset_dict.keys():\n",
    "\n",
    "    trial_dataset_dict[d_idx].filter_reset()\n",
    "    trial_dataset_dict[d_idx].filterout_subjects([0,1,313,314,315,317,318])\n",
    "    trial_dataset_dict[d_idx].filterout_conditions([1])\n",
    "    trial_dataset_dict[d_idx].filter_min_by_session(min_trials = 30)\n",
    "    # trial_dataset_dict[d_idx].filter_lastNdays(n = 5)\n",
    "    data_dict_copy[d_idx] = deepcopy(trial_dataset_dict[d_idx])\n",
    "\n",
    "subjects = trial_dataset_dict[0].metadata_df[trial_dataset_dict[0].metadata_df.keep == True].subject_ID.unique()\n",
    "# quantiles = trial_dataset_dict[0].metadata_df[trial_dataset_dict[0].metadata_df.keep == True].trial_nb_quantile.unique()\n",
    "# quantiles_idx = range(len(quantiles))\n",
    "cmap_quant = cm.get_cmap('jet', len(quantiles))\n",
    "\n",
    "fig, axs = plt.subplots(len(subjects)+1, len(trial_dataset_dict), sharex= 'all',\n",
    "    sharey = 'row', squeeze = False , figsize = figsize)\n",
    "\n",
    "for d_idx, trial_dataset in data_dict_copy.items():\n",
    "    dataset_means = np.empty([len(subjects),len(quantiles),trial_dataset.data.shape[2]])\n",
    "    dataset_sems = np.empty([len(subjects),len(quantiles),trial_dataset.data.shape[2]])\n",
    "\n",
    "    # Only compute for pre-selected trials via the filtering methods\n",
    "    trial_dataset.metadata_df = trial_dataset.metadata_df[trial_dataset.metadata_df.keep == True]\n",
    "    # print(trial_dataset.metadata_df.shape[0],trial_dataset_dict[d_idx].metadata_df.shape[0])\n",
    "    # Compute normalized position in session\n",
    "    trial_dataset.metadata_df = session_nb_normalization(trial_dataset.metadata_df, by_day=False)\n",
    "    trial_dataset.metadata_df = session_nb_quantilization(trial_dataset.metadata_df, quantiles = quantiles)\n",
    "\n",
    "    time_vec = trial_dataset.get_time_vector()\n",
    "\n",
    "    for s_idx, subject in enumerate(subjects):\n",
    "\n",
    "        for q_idx, quantile in enumerate(quantiles):\n",
    "            # indices of the trials to pick\n",
    "            np_idx = trial_dataset.metadata_df[\n",
    "                ((trial_dataset.metadata_df['session_nb_quantile'] ==  q_idx) & (trial_dataset.metadata_df['subject_ID'] == subject))\n",
    "                ].index.values\n",
    "            # mean and sems computation\n",
    "            dataset_means[s_idx,q_idx,:] = np.nanmean(trial_dataset.data[np_idx, trial_dataset.colnames_dict[photo_var], :], axis=0)\n",
    "            dataset_sems[s_idx,q_idx,:] = np.nanstd(trial_dataset.data[np_idx, trial_dataset.colnames_dict[photo_var], :], axis=0) / np.sqrt(len(np_idx))\n",
    "\n",
    "            axs[s_idx, d_idx].fill_between(\n",
    "                time_vec,\n",
    "                dataset_means[s_idx,q_idx,:] - dataset_sems[s_idx,q_idx,:],\n",
    "                dataset_means[s_idx,q_idx,:] + dataset_sems[s_idx,q_idx,:],\n",
    "                alpha=0.2,\n",
    "                color = cmap_quant(q_idx)\n",
    "            )\n",
    "            \n",
    "            axs[s_idx, d_idx].plot(\n",
    "                time_vec, \n",
    "                dataset_means[s_idx,q_idx,:],\n",
    "                alpha=0.7,\n",
    "                color = cmap_quant(q_idx),\n",
    "                label = f'{q_idx+1} (n = {len(np_idx)})',\n",
    "                )\n",
    "            axs[s_idx, d_idx].set_xlim(xlims)\n",
    "            axs[s_idx, d_idx].set_title(subject)\n",
    "            axs[s_idx, d_idx].legend()\n",
    "\n",
    "\n",
    "    for q_idx, quantile in enumerate(quantiles):\n",
    "        \n",
    "        group_sem = np.nanstd(dataset_means[:,q_idx,:],0) / np.sqrt(len(subjects))\n",
    "\n",
    "        axs[s_idx+1, d_idx].fill_between(\n",
    "            time_vec,\n",
    "            np.nanmean(dataset_means[:,q_idx,:],0) - dataset_sems[s_idx,q_idx,:],\n",
    "            np.nanmean(dataset_means[:,q_idx,:],0) + dataset_sems[s_idx,q_idx,:],\n",
    "            alpha=0.2,\n",
    "            color = cmap_quant(q_idx)\n",
    "        )\n",
    "        \n",
    "        axs[s_idx+1, d_idx].plot(\n",
    "            time_vec, \n",
    "            np.nanmean(dataset_means[:,q_idx,:],0),\n",
    "            alpha=0.7,\n",
    "            color = cmap_quant(q_idx),\n",
    "            label = f'{q_idx+1} (n = {len(subjects)})',\n",
    "            )\n",
    "        axs[s_idx+1, d_idx].set_xlim(xlims)\n",
    "        axs[s_idx+1, d_idx].set_title('Group Average')\n",
    "        axs[s_idx+1, d_idx].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import decimate\n",
    "from trialexp.process.pyphotometry.photometry_functional import get_photometry_trials\n",
    "\n",
    "\n",
    "sessions = [session for session in exp_cohort_mixed.sessions if (session.subject_ID == 64 and session.datetime.date() == datetime(2023,2,23).date()) ]\n",
    "\n",
    "df_meta_photo, col_names_numpy, photo_array, photometry_dict = get_photometry_trials(\n",
    "    sessions[1],\n",
    "    conditions_list = None,\n",
    "    cond_aliases = None,\n",
    "    trial_window = [-4000,6000],\n",
    "    trig_on_ev = None,\n",
    "    last_before = None,\n",
    "    baseline_low_pass = 0.001, \n",
    "    low_pass = 45, \n",
    "    median_filt = 5, \n",
    "    motion_corr = True, \n",
    "    df_over_f = True,\n",
    "    z_score = True, # To be implemented\n",
    "    downsampling_factor = 10,\n",
    "    return_full_session = True, \n",
    "    export_vars = ['analog_2','analog_1_df_over_f','analog_1_corrected'],\n",
    "    verbose = False)\n",
    "\n",
    "photometry_dict.keys()\n",
    "\n",
    "\n",
    "vars_to_plot = ['analog_1', 'analog_2', 'analog_1_filt', 'analog_2_filt','analog_1_est_motion', 'analog_1_corrected', 'analog_1_baseline_fluo', 'analog_1_df_over_f', 'zscored_df_over_f']\n",
    "\n",
    "figsize = (20,40)\n",
    "fig, axs = plt.subplots(len(vars_to_plot),1, figsize = figsize)\n",
    "for l, v in enumerate(vars_to_plot):\n",
    "    print(len(photometry_dict[v]) , len(photometry_dict['time']))\n",
    "    if len(photometry_dict[v]) < len(photometry_dict['time']):\n",
    "        axs[l].plot(decimate(photometry_dict['time'],10)/1000, photometry_dict[v])\n",
    "    else:\n",
    "        axs[l].plot(photometry_dict['time']/1000, photometry_dict[v])\n",
    "    axs[l].set_title(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(session.datetime.date(), datetime(2023,2,23).date()) for session in sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigs = [None, 'bar_off', 'bar_off', 'spout', 'US_end_timer']\n",
    "last_befores = [None, None, 'spout', None, None]\n",
    "trial_window = [-4000, 6000]\n",
    "photo_dataset_mixed = dict()\n",
    "photo_dataset_mixed[0] = exp_cohort_mixed.get_photometry_groups(\n",
    "        groups = None, # or use groups variable defined above\n",
    "        conditions_list = conditions_list, \n",
    "        cond_aliases = cond_aliases,\n",
    "        trial_window = trial_window,\n",
    "        when = 'all', \n",
    "        task_names = ['reaching_go_spout_bar_nov22'], #'pavlovian_nobar_nodelay', #'reaching_go_nogo',\n",
    "        trig_on_ev = trig, # align to the first event of a kind e.g. bar_off\n",
    "        last_before = last_befores[idx],\n",
    "        high_pass = 0.01, \n",
    "        low_pass = 45, \n",
    "        median_filt = 3,\n",
    "        motion_corr = True, \n",
    "        df_over_f = True,\n",
    "        z_score = True, \n",
    "        downsampling_factor = 10, \n",
    "        export_vars = ['analog_1_df_over_f', 'zscored_df_over_f'], \n",
    "        # remove_artifacts = False, # To Deprecate in favor of Exp level artifact clustering\n",
    "        verbose = True) # will plot all the process of remove_artifacts if remove_artifacts == True\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trialexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6482735f0043a190ffe4caf0b320db79ae95bb9719ede4e9819067a592cbc72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
